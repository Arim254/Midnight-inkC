[
  {
    "title": "LG’s new OLED TV is just 9mm thick",
    "url": "https://techcrunch.com/2026/01/05/lgs-new-oled-tv-is-just-9mm-thick/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/LG-OLED-evo-W6_main.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T12:13:11.000Z",
    "description": "A significant portion of the annual Consumer Electronics Show (CES) is about TVs, and this year, LG is showing off its manufacturing chops with a new Wallpaper OLED TV that is just 9mm thick. The South Korean...",
    "body": "\nA significant portion of the annual Consumer Electronics Show (CES) is about TVs, and this year, LG is showing off its manufacturing chops with a new Wallpaper OLED TV that is just 9mm thick.\nThe South Korean company launched the Wallpaper line in 2017, and is now bringing it back with this model, dubbed OLED evo W6. The company says you can connect the TV to its Zero Connect Box wirelessly to stream lossless 4K video and audio, provided the box is within 10 meters of the TV.\nLG claims the TV improves on brightness, color, and blacks compared to its predecessor, and is certified “reflection-free” by product testing group Intertek. The display has a refresh rate maxing out at 165Hz, and supports AMD’s FreeSync Premium tech.\nLG didn’t provide any pricing details, but it said that the W6 will be available in 77- and 83-inch sizes.\n\n\tTopics\n"
  },
  {
    "title": "Narwal adds AI to its vacuum cleaners to monitor pets and find jewelry",
    "url": "https://techcrunch.com/2026/01/05/narwal-adds-ai-to-its-vacuum-cleaners-to-monitor-pets-and-find-jewelry/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/002.jpeg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T07:18:00.000Z",
    "description": "Image Credits:Narwal\t \t \t\t\t\t\t\t11:18 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t  \t\t\t Robot vacuum maker Narwal unveiled its new set of smart vacuum cleaners at the Consumer Electronics Show (CES) with AI-powered...",
    "body": "\n\t\n\t\tImage Credits:Narwal\t\n\t\n\t\t\t\t\t\t11:18 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nRobot vacuum maker Narwal unveiled its new set of smart vacuum cleaners at the Consumer Electronics Show (CES) with AI-powered features such as monitoring pets, finding valuable objects, and notifying users about misplaced toys.\nThe company said that its new flagship Flow 2 robot vacuum has a rounded design and easy-lift tanks for better cleaning. The device uses two 1080p RGB cameras with a 136-degree field of view to map out the area and recognize different kinds of objects using AI models.\nNarwal said that through this tech stack, the vacuum cleaner has the ability to identify an unlimited number of objects. The device first tries to identify an object locally, but in case there are no matches, it sends the data to the cloud for further processing. \nImage Credits: Narwal\nThe Flow 2 has three key modes called pet care mode, baby care mode, and AI floor tag mode. With pet care mode, you can define zones where pets usually rest or hang out to clean them. Plus, it can monitor pets and also check in on your pets via two-way audio (there is no guarantee that they would listen to you, though). In the baby care mode, the vacuum switches to quiet mode near the crib and notifies you of misplaced toys. In the AI floor tag mode, the vacuum recognizes valuable items like jewelry, avoids them, and alerts you.\nNarwal said that its newest vacuum cleaner has four cleaning modes that can identify different types of dirt. The device can also return to its base to wash the mop and then re-mop a certain area if it is dirty. The company noted that the Flow 2’s design allows for a higher hot water washing temperature for better cleaning.\nImage Credits: Narwal\nAlong with the Flow 2, the company also showed off a handheld vacuum called the U50 that weighs 1.41kg (3.1 lbs) and has UV-C sterilization along with heat treatment for allergen removal. The company also demoed an unnamed cordless vacuum with a slim design, 360-degree swivel, and up to 50 minutes of run time. The cordless vacuum also has an auto-empty station that can support up to 60 days of dust disposal.\n\n\t\t\t\n\tTopics\n\n\n\n\t\tIvan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web.\r\nYou can contact or verify outreach from Ivan by emailing im@ivanmehta.com or via encrypted message at ivan.42 on Signal.\t\n\t\n\t\tView Bio \n\t\n\n\t\t"
  },
  {
    "title": "Lucid Motors doubled EV output in 2025 after early Gravity SUV struggles",
    "url": "https://techcrunch.com/2026/01/05/lucid-motors-doubled-ev-output-in-2025-after-early-gravity-suv-struggles/",
    "image": "https://techcrunch.com/wp-content/uploads/2025/04/lucid-gravity-main.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T16:51:18.000Z",
    "description": "Image Credits:Lucid Group\t \t \t\t\t\t\t\t8:51 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t  \t\t\t Lucid Motors built twice as many electric vehicles in 2025 as it did in the prior year, a sign that the company has bounced...",
    "body": "\n\t\n\t\tImage Credits:Lucid Group\t\n\t\n\t\t\t\t\t\t8:51 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLucid Motors built twice as many electric vehicles in 2025 as it did in the prior year, a sign that the company has bounced back from early production struggles with its new Gravity SUV. \nThe company announced Monday that it finished the year having built 18,378 EVs, with 8,412 of those coming in the fourth quarter alone. That’s more than Lucid built at its Casa Grande, Arizona factory in the first half of the year. Lucid also said it delivered — meaning sold — 15,841 vehicles across the whole year, a 55% increase over 2024’s figures.\nThe stronger finish to 2025 sets Lucid up for an all-important year that will see the company start building the first vehicle on its new mid-sized EV platform. The company has said this first vehicle will cost around $50,000, putting it near the same part of the market as the Tesla Model Y and Rivian’s upcoming R2 SUV.\nThe numbers still pale in comparison to the projections Lucid Motors threw around when it went public in a $4 billion reverse merger in 2021. At that time, the company claimed it would deliver 135,000 vehicles in 2025, with 86,000 of those being Gravity SUVs, 42,000 being Air sedans, and the remaining 7,000 coming from its yet-to-debut mid-sized EV. \nThose targets quickly became unrealistic as Lucid ran into production, supply, and demand challenges for both of its vehicles, all while navigating an automotive market that was severely disrupted by the pandemic. The company particularly struggled in early 2025 as it started to ramp up production of the Gravity SUV. It has since been dealing with a number of quality issues on the vehicle, to the level that interim CEO Marc Winterhoff sent an email to customers in December saying that he shared in their “frustration.”\n“Lingering software problems have unfortunately affected our customers’ experience and satisfaction. I would like to assure you that we are laser focused on addressing these issues,” he wrote.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\n\n\t\t\t\n\tTopics\n\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n\n\t\t"
  },
  {
    "title": "AMD unveils new AI PC processors for general use and gaming at CES",
    "url": "https://techcrunch.com/2026/01/05/amd-unveils-new-ai-pc-processors-for-general-use-and-gaming-at-ces/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Ryzen-AI-Blog-1200x675-1.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T03:30:00.000Z",
    "description": "AMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone. As part of that promise, AMD announced a new line of AI processors as the...",
    "body": "\nAMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone. \nAs part of that promise, AMD announced a new line of AI processors as the company thinks AI-powered personal computers are the way of the future.\nThe semiconductor giant revealed AMD Ryzen AI 400 Series processor, its latest version of its AI-powered PC chips, at the yearly CES conference on Monday. The company says the latest version of its Ryzen processor series allows for 1.3x faster multitasking than its competitors and are 1.7x times faster at content creation.\nThese new chips feature 12 CPU Cores, individual processing units inside a core processor, and 24 threads, independent streams of instruction\nThis is an upgrade to the Ryzen AI 300 Series processor that was announced in 2024. AMD started producing the Ryzen processor series in 2017.\nRahul Tikoo, senior vice president and general manager of AMD’s client business, said AMD has expanded to over 250 AI PC platforms on the company’s recent press briefing. That represents a growth 2x over the last year, he added.\n“In the years ahead, AI is going to be a multi-layered fabric that gets woven into every level of computing at the personal layer,” Tikoo said. “Our AI PCs and devices will transform how we work, how we play, how we create and how we connect with each other.”\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAMD also announced the release of the AMD Ryzen 7 9850X3D, the latest version of its gaming-focused processor.\n“No matter who you are and how you use technology on a daily basis, AI is reshaping everyday computing,” Tikoo said. “You have thousands of interactions with your PC every day. AI is able to understand, learn context, bring automation, provide deep reasoning and personal customization to every individual.”\nPCs that include either the Ryzen AI 300 Series processor or the AMD Ryzen 7 9850X3D processor become available in the first quarter of 2026.\nThe company also announced the latest version of its Redstone ray tracing technology, which simulates physical behavior of light, which allows for better video game graphics without a performance or speed lag.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here. \n\n\n\t\tBecca is a senior writer at TechCrunch that covers venture capital trends and startups. She previously covered the same beat for Forbes and the Venture Capital Journal.\r\nYou can contact or verify outreach from Becca by emailing rebecca.szkutak@techcrunch.com.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "Insight Partners sued by former vice president Kate Lowry",
    "url": "https://techcrunch.com/2026/01/05/insight-partners-sued-by-former-vice-president-kate-lowry/",
    "image": "https://techcrunch.com/wp-content/uploads/2023/11/gavel-messy-legal.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:30:00.000Z",
    "description": "Kate Lowry, a former vice president at Insight Partners, is suing the firm, alleging disability discrimination, gender discrimination, and wrongful termination, according to a suit filed on December 30 in San...",
    "body": "\nKate Lowry, a former vice president at Insight Partners, is suing the firm, alleging disability discrimination, gender discrimination, and wrongful termination, according to a suit filed on December 30 in San Mateo County, California, and seen by TechCrunch. Insight Partners did not immediately respond to TechCrunch’s request for comment.\nLowry told TechCrunch she filed the suit because she believes “too many powerful, wealthy people in venture act like it’s OK to break the law and systemically underpay and abuse their employees.”\n“It’s an oppressive system that reflect[s] broader trends in society that use fear, intimidation, and power to silence and isolate truth. I’m trying to change that.”\nLowry began working at Insight Partners in 2022, after previously working for Meta, McKinsey & Company, and an early-stage startup. The suit alleges that, upon being hired, she was assigned to a different supervisor than the person mentioned during her interview.  \nShe alleges in the suit that she was told by her new supervisor, who was a woman, to be “online all the time, including PTO, holidays, and weekends,” and to respond between “6 a.m. and 11 p.m. daily.”  \nLowry says in the suit that this first supervisor “berated, hazed, and antagonized” her, spoke openly about a hazing that would be “longer and more intense” than what she put other male reports through.  \nSome comments the supervisor allegedly made, according to the suit, include “you are incompetent, shut up and take notes” and “you need to obey me like a dog; do whatever I say whenever I say it, without speaking.”  Lowry also alleges that her supervisor assigned her “redundant tasks” and restricted her ability to participate in calls, while allowing less experienced male colleagues to do so. Lowry, instead, she alleges, was relegated to “administrative tasks such as note-taking and cataloging.”  \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nLowry said she became “increasingly ill” because of the work environment and that her physician advised a medical leave of absence, which she was granted and took from February to July 2023.  \nWhen she returned to work, she was placed on a new team and, the suit alleges, was told by the head of human resources that “if the new team did not like her, she would be fired.”  \nIn September 2023, Lowry said she got a concussion and took another medical leave and returned to work near the end of 2024. Due to some departures, she was placed under the supervision of a new person, where Lowry said her poor treatment continued. She also alleges that in 2024, her compensation was about 30% below the market. \nBy April 2025, she alleges she was told her compensation would be cut. In May of 2025, through her attorneys, Lowry sent a letter to Insight regarding her alleged treatment by the company. A week later, the firm terminated her employment, the suit states.  \nThe lawsuit is reminiscent of Ellen Pao’s suit against Kleiner Perkins back in 2012, in which she alleged discrimination and retaliation. That suit offered what was, at the time, a rare glimpse into how women partners felt they were treated in venture capital. Though Pao lost that suit, it sent waves through the industry, and other women went on to sue major tech companies.  \n\n\n\t\tDominic-Madori Davis is a senior venture capital and startup reporter at TechCrunch. She is based in New York City.\r\nYou can contact or verify outreach from Dominic by emailing dominic.davis@techcrunch.com or via encrypted message at +1 646 831-7565 on Signal.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "Nvidia wants to be the Android of generalist robotics ",
    "url": "https://techcrunch.com/2026/01/05/nvidia-wants-to-be-the-android-of-generalist-robotics/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-05-at-5.03.42-PM.png?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:00:00.000Z",
    "description": "Nvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to become the default platform for generalist robotics, much...",
    "body": "\nNvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to become the default platform for generalist robotics, much as Android became the operating system for smartphones. \nNvidia’s move into robotics reflects a broader industry shift as AI moves off the cloud and into machines that can learn how to think in the physical world, enabled by cheaper sensors, advanced simulation, and AI models that increasingly can generalize across tasks. \nNvidia revealed details on Monday about its full-stack ecosystem for physical AI, including new open foundation models that allow robots to reason, plan, and adapt across many tasks and diverse environments, moving beyond narrow task-specific bots, all of which are available on Hugging Face. \nThose models include: Cosmos Transfer 2.5 and Cosmos Predict 2.5, two world models for synthetic data generation and robot policy evaluation in simulation; Cosmos Reason 2, a reasoning vision language model (VLM) that allows AI systems to see, understand, and act in the physical world; and Isaac GR00T N1.6, its next-gen vision language action (VLA) model purpose-built for humanoid robots. GR00T relies on Cosmos Reason as its brain, and it unlocks whole-body control for humanoids so they can move and handle objects simultaneously. \nNvidia also introduced Isaac Lab-Arena at CES, an open source simulation framework hosted on GitHub that serves as another component of the company’s physical AI platform, enabling safe virtual testing of robotic capabilities.\nThe platform promises to address a critical industry challenge: As robots learn increasingly complex tasks, from precise object handling to cable installation, validating these abilities in physical environments can be costly, slow, and risky. Isaac Lab-Arena tackles this by consolidating resources, task scenarios, training tools, and established benchmarks like Libero, RoboCasa, and RoboTwin, creating a unified standard where the industry previously lacked one.\nSupporting the ecosystem is Nvidia OSMO, an open source command center that serves as connective infrastructure that integrates the entire workflow from data generation through training across both desktop and cloud environments. \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAnd to help power it all, there’s the new Blackwell-powered Jetson T4000 graphics card, the newest member of the Thor family. Nvidia is pitching it as a cost-effective on-device compute upgrade that delivers 1200 teraflops of AI compute and 64 gigabytes of memory while running efficiently at 40 to 70 watts. \nNvidia is also deepening its partnership with Hugging Face to let more people experiment with robot training without needing expensive hardware or specialized knowledge. The collaboration integrates Nvidia’s Isaac and GR00T technologies into Hugging Face’s LeRobot framework, connecting Nvidia’s 2 million robotics developers with Hugging Face’s 13 million AI builders. The developer platform’s open source Reachy 2 humanoid now works directly with Nvidia’s Jetson Thor chip, letting developers experiment with different AI models without being locked into proprietary systems.  \nThe bigger picture here is that Nvidia is trying to make robotics development more accessible, and it wants to be the underlying hardware and software vendor powering it, much like Android is the default for smartphone makers.\nThere are early signs that Nvidia’s strategy is working. Robotics is the fastest growing category on Hugging Face, with Nvidia’s models leading downloads. Meanwhile robotics companies, from Boston Dynamics and Caterpillar to Franka Robots and NEURA Robotics, are already using Nvidia’s tech.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here.\n\n\n\n\t\tRebecca Bellan is a senior reporter at TechCrunch where she covers the business, policy, and emerging trends shaping artificial intelligence. Her work has also appeared in Forbes, Bloomberg, The Atlantic, The Daily Beast, and other publications. \r\nYou can contact or verify outreach from Rebecca by emailing rebecca.bellan@techcrunch.com or via encrypted message at rebeccabellan.491 on Signal.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "Lego Smart Bricks introduce a new way to build — and they don’t require screens",
    "url": "https://techcrunch.com/2026/01/05/lego-smart-bricks-introduce-a-new-way-to-build-and-they-dont-require-screens/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/SMARTBrick_16x9.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T20:31:51.000Z",
    "description": "Image Credits:Lego\t \t \t\t\t\t\t\t12:31 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t  \t\t\t Lego announced its new Smart Play system at CES 2026 on Monday, adding interactive, responsive Legos to the famously analog...",
    "body": "\n\t\n\t\tImage Credits:Lego\t\n\t\n\t\t\t\t\t\t12:31 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLego announced its new Smart Play system at CES 2026 on Monday, adding interactive, responsive Legos to the famously analog franchise.\nThe Smart Play system includes a 2×4 brick, Smart Tag tiles, and Smart Minifigures. The Smart Bricks and Minifigures can sense nearby Smart Tags, which are 2×2 studless tiles with unique digital IDs that tell the Bricks and Minifigures how to act.\nIf the Smart Tag comes in a set for building a helicopter, for example, then the Smart Brick will light up and make propeller sounds that would help bring a helicopter to life. Its built-in accelerometer would make these lights and sounds more consistent with how you’re actually playing with the helicopter, since the Brick will be able to sense when the helicopter is zooming through the sky or turned upside down.\nImage Credits:LEGO\nThe Smart Bricks are powered by a patented ASIC chip, which is smaller than the size of a single Lego stud. The chip uses near-field magnetic positioning to recognize the Tags around it, as well as a miniature speaker, accelerometer, and LED array. Lego also developed a Bluetooth-based protocol called BrickNet, which allows multiple Smart Bricks to recognize each other and operate in tandem. The company claims that BrickNet is protected by enhanced encryption and privacy controls (all of which is necessary, but imagine a world where hacking into toys wasn’t a concern!).\nThere’s no setup required to pair the elements of the Smart Play system, making it easy for kids to get started — and parents will be pleased to note that there are no screens involved in the Smart system at all. However, Lego’s website says that there will be a Smart Tag for animating Lego toilets, so… there’s that.\nLego’s first two Smart Play sets — which are both Star Wars-themed — will launch on March 1, though preorders open on Friday. The “Luke’s Red Five X-wing” building set will retail for $69.99, while the larger “Throne Room Duel and A-wing” set will cost $159.99. These sets use the Smart Play system to animate characters like Luke Skywalker and Princess Leia, allowing them to interact with Smart Tags, which enable Lightsaber duels among other Star Wars-related capabilities.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\n\n\t\t\t\n\tTopics\n\n\n\n\t\tAmanda Silberling is a senior writer at TechCrunch covering the intersection of technology and culture. She has also written for publications like Polygon, MTV, the Kenyon Review, NPR, and Business Insider. She is the co-host of Wow If True, a podcast about internet culture, with science fiction author Isabel J. Kim. Prior to joining TechCrunch, she worked as a grassroots organizer, museum educator, and film festival coordinator. She holds a B.A. in English from the University of Pennsylvania and served as a Princeton in Asia Fellow in Laos.\nYou can contact or verify outreach from Amanda by emailing amanda@techcrunch.com or via encrypted message at @amanda.100 on Signal. \t\n\t\n\t\tView Bio \n\t\n\n\t\t"
  },
  {
    "title": "Boston Dynamics’ next-gen humanoid robot will have Google DeepMind DNA",
    "url": "https://techcrunch.com/2026/01/05/boston-dynamicss-next-gen-humanoid-robot-will-have-google-deepmind-dna/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/atlas-announcement.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T21:55:55.000Z",
    "description": "Robotics company Boston Dynamics has struck a partnership with Google’s AI research lab to speed up the development of its next-generation humanoid robot Atlas — and make it act more human around people. The...",
    "body": "\nRobotics company Boston Dynamics has struck a partnership with Google’s AI research lab to speed up the development of its next-generation humanoid robot Atlas — and make it act more human around people.\nThe partnership, which was announced Monday during the Hyundai press conference at CES 2026, is centered on robotics research that will use Google DeepMind’s AI foundation models. Boston Dynamics’ humanoid robot Atlas will be the first test case, according to Carolina Parada, senior director of robotics at Google DeepMind.\n“We’re looking to integrate our cutting-edge AI foundation models with Boston Dynamics’ new Atlas robots, and we’ll aim to develop the world’s most advanced robot foundation model to fulfill the promise of true general-purpose human needs,” Parada said onstage.\nThe tie-up comes less than a year after the Google AI research lab announced new AI models called Gemini Robotics that are designed to allow robots to perceive, reason, use tools, and interact with humans. Gemini Robotics is based on a large-scale multimodal generative AI model, Gemini. At the time, Google DeepMind said the robotics AI model was trained to generalize behavior across a range of different robotics hardware.\nEnter Boston Dynamics, and its majority owner, Hyundai Motor Group. While accelerating research will be a central piece of this partnership, this has real-world scaling intent.\nBoston Dynamics already has products, like the quadruped Spot, that are in customers’ hands in more than 40 countries. Its warehouse robot Stretch has unloaded more than 20 million boxes globally since its launch in 2023, according to Hyundai. Now Boston Dynamics and Hyundai are preparing for the next generation, starting with the humanoid robot Atlas, which the company announced Monday is already in production and headed to the Hyundai factory in Savannah, Georgia\nA prototype of Atlas walked onstage during the press conference, showing off its ability to move. But as Alberto Rodriguez, director of Atlas behavior at Boston Dynamics, noted, making “Atlas into a product requires more than athletic performance for humanoids to really deliver on their promise. They have to be able to interact with people naturally.”\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nRodriguez and his counterparts at Boston Dynamics believe that recent advancements in AI have created a clear path to get to those capabilities. That kind of natural interaction with humans has real safety implications. \nThe Atlas product, which was also revealed onstage Monday and will eventually head to Hyundai’s factory, has 56 degrees of freedom with rotation joints and human-scale hands that have tactile sensing. And it’s strong. The Atlas robot can lift up to 110 pounds and is designed to perform repetitive movements. \nWith that kind of dexterity and strength, it will be critical for Atlas, or any humanoid robot, to safely interact and work with humans. Some of that has been handled on the hardware side; Atlas, for instance, has 360-degree cameras to allow it to see when people are approaching. But DeepMind’s work could help the robots learn how to act.\n“Rather than having a set of predefined, loaded tasks onto the robot, we think robots should understand the physical world the same way we do,”Parada said. “They should be able to learn from their experience. Should be able to generalize new situations and get better over time. So whether it is to assemble a new car part or to tie your shoelaces, robots should learn the same way we do from a handful of examples, and then get better very quickly with a little bit of practice.”\nHyundai, which plans to bring Atlas to its factory this year and eventually deploy them for tasks like parts sequencing by 2028, has also developed protocols to increase safety and efficiency.\nHyundai said Monday it is opening a U.S. facility this year called a Robot Metaplant Application Center, or RMAC, that will teach robots how to map movements like lifts and turns. Training data from RMAC will be combined with real-world data collected via a software platform used in its Georgia factory to continually improve the robots.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here. \nThis article was updated to include more information about Atlas’ specs. \n\n\n\t\tKirsten Korosec is a reporter and editor who has covered the future of transportation from EVs and autonomous vehicles to urban air mobility and in-car tech for more than a decade. She is currently the transportation editor at TechCrunch and co-host of TechCrunch’s Equity podcast. She is also co-founder and co-host of the podcast, “The Autonocast.” She previously wrote for Fortune, The Verge, Bloomberg, MIT Technology Review and CBS Interactive.\nYou can contact or verify outreach from Kirsten by emailing kirsten.korosec@techcrunch.com or via encrypted message at kkorosec.07 on Signal.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "Google previews new Gemini features for TV at CES 2026",
    "url": "https://techcrunch.com/2026/01/05/google-previews-new-gemini-features-for-tv-at-ces-2026/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/google-tv.webp?w=150",
    "tag": "Tech",
    "date": "2026-01-05T14:00:00.000Z",
    "description": "Google believes AI can improve the TV-watching experience, which is why it brought its Gemini AI to Google TV devices in November. At CES 2026 in Las Vegas, the company is now showing off a series of new...",
    "body": "\nGoogle believes AI can improve the TV-watching experience, which is why it brought its Gemini AI to Google TV devices in November. At CES 2026 in Las Vegas, the company is now showing off a series of new Gemini features that will soon arrive on the TV, making it possible for viewers to deep-dive into topics, search for and “reimagine” their personal photos and videos with AI, and, perhaps best of all, tell the TV what to do instead of having to navigate through complicated settings.\nThe company is first bringing these Gemini features and others to select TCL televisions before rolling them out more broadly to other Google TV devices in the months ahead. \nDesigned for large-screen experiences, Gemini for Google TV will allow users to talk to their TV to find something to watch, catch up on a favorite series by getting a recap of the plot, or get recommendations, all by using natural language conversation. For instance, you could ask Gemini for something to watch that would be a blend of two people’s tastes, or get help remembering a show or movie where you can’t remember the title, but can describe the plot or name one of the actors. \nYou could even ask Google something like, “What’s the new hospital drama everyone’s talking about?”\nImage Credits:Google\nGemini can respond to users’ questions through a new visually rich framework that adapts to individual queries, combining text, imagery, video context, and real-time sports updates, as required. \nBut Google sees the potential for the TV’s screen to be used for more than just entertainment; with Gemini, the TV can be used to educate, too. \nAt CES, Google showed how this would work. When users ask a question about something they want to learn about, the TV screen can offer a deep dive into the topic. A narrated interactive overview simplifies concepts, and users can ask follow-up questions to learn more.\nImage Credits:Google\nGemini will also allow users to query their Google Photos library for specific people or moments. They can also apply artistic styles to their photos and videos using Gemini AI and turn their memories into cinematic slideshows, says Google.\nImage Credits:Google\nImage Credits:Google\nHowever, perhaps the most useful feature is one that gives you the power to optimize the TV’s settings using only your voice.\nNow, you’ll be able to tell Gemini things like “the screen is too dim” or “I can’t hear the dialogue,” and Gemini will adjust the relevant settings without you having to leave the movie or TV show you’re watching to dig through menus to find the necessary option. \nImage Credits:Google\nGoogle says the new Gemini features will require the Google TV devices to be running Android TV OS 14 or higher, and they will need an internet connection. Not all languages, countries, or devices will be supported at launch, and users must also have a Google account to access the Gemini for TV experience. \n\n\n\t\tSarah has worked as a reporter for TechCrunch since August 2011. She joined the company after having previously spent over three years at ReadWriteWeb. Prior to her work as a reporter, Sarah worked in I.T. across a number of industries, including banking, retail and software.\nYou can contact or verify outreach from Sarah by emailing sarahp@techcrunch.com or via encrypted message at sarahperez.01 on Signal. \t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "Offshore wind developers sue Trump administration for halting $25B in projects",
    "url": "https://techcrunch.com/2026/01/05/offshore-wind-developers-sue-trump-administration-for-halting-25b-in-projects/",
    "image": "https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-831764682.jpeg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T18:20:23.000Z",
    "description": "Three offshore wind developers are suing the Trump administration after the Department of the Interior halted five projects worth a total of $25 billion on December 22. If completed, the projects would...",
    "body": "\nThree offshore wind developers are suing the Trump administration after the Department of the Interior halted five projects worth a total of $25 billion on December 22. If completed, the projects would generate a total of 6 gigawatts of electricity.\nTwo lawsuits were filed Thursday and Friday last week by Ørsted and Equinor, which are developing the 704 megawatt Revolution Wind and the 2 gigawatt Empire Wind, respectively. Another was filed on December 23 by Dominion Energy, which is building a 2.6 gigawatt farm off the coast of Virginia.\nRevolution Wind is nearly 90% complete, while Empire Wind and Coastal Virginia Offshore Wind are each about 60% complete. Dominion said it was losing $5 million per day as a result of the halt.\nAvangrid, which is developing Vineyard Wind 1, has not filed a lawsuit yet. Nearly half that project is currently operational.\nThe Department of the Interior cited national security concerns in its decision to stop construction on the projects. Though it didn’t mention specifics, the Trump administration may have been referencing the challenges wind turbines present to radar operations. The Department of Energy had issued a report that discussed this security concern, as well as solutions to it, in February 2024.\nWind turbines’ whirling blades have been known to stymie radar systems, but researchers in the government and private companies have been working to mitigate the problem for well over a decade. \nChoosing the precise site for wind energy projects is one of the biggest ways to ameliorate interference. The Bureau of Ocean Energy Management coordinates with the Military Aviation and Installation Assurance Siting Clearinghouse to “review each proposed offshore wind project on a project-by-project basis, and would attempt to de-conflict concerns related to individual projects or multiple projects,” according to Vineyard Wind 1’s environmental impact statement.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nNewer radar systems can filter the noise that wind farms produce through adaptive processing algorithms, Rand Corporation senior engineer Nicholas O’Donoughue previously told TechCrunch. Vineyard Wind 1 agreed to help fund radar adaptations and to curtail operations when asked by the Pentagon, for example. \nEarlier last year, the Trump administration halted approvals for new offshore wind projects in addition to pausing work on Empire Wind and Revolution Wind. The latter restarted after New York State negotiated with the Trump administration, while a federal judge struck down the stop work order for Revolution Wind. \n\n\n\t\tTim De Chant is a senior climate reporter at TechCrunch. He has written for a wide range of publications, including Wired magazine, the Chicago Tribune, Ars Technica, The Wire China, and NOVA Next, where he was founding editor. \r\nDe Chant is also a lecturer in MIT’s Graduate Program in Science Writing, and he was awarded a Knight Science Journalism Fellowship at MIT in 2018, during which time he studied climate technologies and explored new business models for journalism. He received his PhD in environmental science, policy, and management from the University of California, Berkeley, and his BA degree in environmental studies, English, and biology from St. Olaf College.\r\nYou can contact or verify outreach from Tim by emailing tim.dechant@techcrunch.com.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "Hacktivist deletes white supremacist websites live onstage during hacker conference",
    "url": "https://techcrunch.com/2026/01/05/hacktivist-deletes-white-supremacist-websites-live-on-stage-during-hacker-conference/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/martha-root-whitedate-hack-screenshot1-e1767638022105.png?w=150",
    "tag": "Tech",
    "date": "2026-01-05T18:57:31.000Z",
    "description": "A hacker known as Martha Root broke in and deleted three white supremacist websites at the end of a talk during the annual hacker conference Chaos Communication Congress in Germany.",
    "body": "\nA hacktivist remotely wiped three white supremacist websites live onstage during their talk at a hacker conference last week, with the sites yet to return online.\nThe pseudonymous hacker, who goes by Martha Root — dressed as Pink Ranger from the Power Rangers — deleted the servers of WhiteDate, WhiteChild, and WhiteDeal in real time at the end of a talk at the annual Chaos Communication Congress in Hamburg, Germany. \nRoot gave the talk alongside journalists Eva Hoffmann and Christian Fuchs, who wrote an article about the hacked sites for the German weekly paper Die Zeit in October. \nAs of this writing, WhiteDate, which Hoffmann described as a “Tinder for Nazis”; WhiteChild, a site that claimed to match white supremacists’ sperm and egg donors; and WhiteDeal, a sort-of Taskrabbit-esque labor marketplace for racists, are all offline.\nThe administrator of the three websites confirmed the hack on their social media accounts. \n“They publicly delete all my websites while the audience rejoices. This is cyberterrorism,” the administrator wrote on X on Sunday, vowing repercussions.\nThe administrator also claimed that Root deleted their X account before it was restored.\n\n‼️A German hacker known as \"Martha Root\" dressed as a pink Power Ranger and deleted a white supremacist dating website live onstageThis happened during the recent CCC conference.Martha had infiltrated the site, ran her own AI chatbot to extract as much information from users… pic.twitter.com/vpTEoFR8JR— International Cyber Digest (@IntCyberDigest) January 2, 2026\n\nRoot also published the data allegedly scraped from WhiteDate online. \nThe hacker said that they scraped WhiteDate’s public data and found “poor cybersecurity hygiene that would make even your grandma’s AOL account blush.” Root said that users’ images included precise geolocation metadata that “practically hands out home addresses with a side of awkward selfies.” \n“Imagine calling yourselves the ‘master race’ but forgetting to secure your own website — maybe try mastering to host WordPress before world domination,” Root wrote. \nThe leaked data includes users’ profiles with name, pictures, description, age, location (both containing precise coordinates and user-set country and state), gender, language, race, and other personal information that users uploaded. Root wrote on the site that “for now” there are no emails, passwords, or private conversations. \nAccording to the leaked data, WhiteData had more than 6,500 users, of which 86% were men and 14% women. “A gender ratio that makes the Smurf village look like a feminist utopia,” Root wrote.\nRoot infiltrated the sites using AI chatbots that bypassed verification processes and were verified as “white,” according to the talks’ abstract. \nDDoSecrets, a nonprofit collective that stores leaked datasets in the public interest, announced that it has received “files and user information” from the three white supremacist websites. The collective, which calls this release “WhiteLeaks,” has not publicly released the data but is instead asking verified journalists and researchers to request access to the full 100 gigabyte dataset.\nThe administrator of the three websites did not immediately respond to TechCrunch’s request for comment, which was sent to an email address shown during the conference talk. TechCrunch also sent an email to an address that appears on the public domain records of two of the three websites. The person behind that address also did not immediately respond to our email.\nRoot, Hoffmann, and Fuchs claim to have identified the real identity of the websites’ administrator as a woman from Germany. TechCrunch could not independently confirm the identity of the administrator.\n\n\n\t\tLorenzo Franceschi-Bicchierai is a Senior Writer at TechCrunch, where he covers hacking, cybersecurity, surveillance, and privacy. \nYou can contact or verify outreach from Lorenzo by emailing lorenzo@techcrunch.com, via encrypted message at +1 917 257 1382 on Signal, and @lorenzofb on Keybase/Telegram.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "China’s Hesai will double production as lidar sensor industry shakes out",
    "url": "https://techcrunch.com/2026/01/05/chinas-hesai-will-double-production-as-lidar-sensor-industry-shakes-out/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/image.png?w=150",
    "tag": "Tech",
    "date": "2026-01-05T18:00:00.000Z",
    "description": "Chinese lidar-maker Hesai announced plans on Monday to double its production capacity from 2 million units to 4 million units this year, as it looks to corner the global market for the laser-based sensors....",
    "body": "\nChinese lidar-maker Hesai announced plans on Monday to double its production capacity from 2 million units to 4 million units this year, as it looks to corner the global market for the laser-based sensors. That would be well up from the 1 million-plus unit mark that Hesai hit in 2025.\nHesai’s push to grab more market share comes just one month after leading U.S. lidar-maker Luminar filed for Chapter 11 bankruptcy. That company is not expected to continue operating once its bankruptcy plan is approved, though it is looking to sell the lidar business.\nHesai has raised hundreds of millions of dollars over the last few years and is now listed on both the Nasdaq and the Hong Kong stock exchanges. That’s despite fighting an uphill battle against the U.S. government, which has accused the company of working closely with China’s military industry — a charge that Hesai has challenged.\nAt the 2026 Consumer Electronics Show in Las Vegas, Hesai told reporters it was able to double the production target because of “accelerating demand” in the automotive and robotics industries.\nThe company’s automotive efforts have been buoyed by the Chinese car market’s adoption of lidar sensors, which Hesai said is now in 25% of new electric cars sold in the country. It also claimed that many new vehicles in China are expected to integrate between three to six lidar sensors per car, “significantly expanding Hesai’s addressable market.” Hesai boasts 24 automotive customers, including a “top European” automaker, and said it has 4 million orders for its newest ATX lidar sensor.\nAutomotive has proved to be a fickle market for lidar sensors outside of China. That was one of the contributing factors to Luminar’s downfall, according to the company’s own bankruptcy filings. While Luminar secured deals to integrate its lidar sensors on Volvo, Polestar, and Mercedes-Benz vehicles, those plans fell apart. Volvo had at one point agreed to buy 1.1 million lidar sensors from Luminar, but delays to its new vehicle programs and cost overruns caused the Swedish automaker to back out of the deal. (Volvo ultimately only purchased around 10,000 sensors from Luminar.)\nRobotics is not guaranteed to be a successful market for lidar sensors, but some players besides Hesai see great promise. San Francisco-based Ouster, which acquired rival player Velodyne in 2023 as the lidar industry began to consolidate, has said it believes robotics represents a $14 billion market opportunity. This includes not just humanoid robotics, but last-mile delivery robots and military applications.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAt CES, Hesai is showcasing a robotic lawnmower and a robotic dog that use the company’s JT series lidar sensor. The company also hinted at its inclusion in humanoid robots as well. It has struck deals to provide lidar sensors to autonomous vehicle companies like Pony AI, Motional, WeRide, and Baidu.\nHesai also boasted it has helped drive down the cost of lidar sensors by 99.5% in just eight years. That, too, was a contributing factor to Luminar’s downfall; “pressure to reduce costs due to lower price points of China-based competitors” has been regularly listed in the company’s bankruptcy filings as the second-most important factor that explains why the U.S. company found it so hard to build up a self-sustaining business.\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "Luminar claims founder Austin Russell is dodging a subpoena in the bankruptcy case",
    "url": "https://techcrunch.com/2026/01/05/luminar-claims-founder-austin-russell-is-dodging-a-subpoena-in-the-bankruptcy-case/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-1741630514.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T12:10:00.000Z",
    "description": "Lidar-maker Luminar says its founder and former CEO Austin Russell has been evading requests for information — including a subpoena — that the company needs in order to decide whether it should take legal...",
    "body": "\nLidar-maker Luminar says its founder and former CEO Austin Russell has been evading requests for information — including a subpoena — that the company needs in order to decide whether it should take legal action against him. \nThe company, which entered the Chapter 11 bankruptcy process in late December, said in an emergency filing over the weekend that it has been trying to reclaim company-owned devices from Russell since his resignation in May. While it has recovered six computers, Luminar is still seeking Russell’s company-issued phone and a digital copy of his personal phone.\nLuminar’s lawyers also wrote in the filing that Russell and his own personal employees repeatedly misled legal representatives about the founder’s location over the holidays. They’re asking the court for permission to instead serve Russell by mail or email. A lawyer for Luminar declined to comment further.\nIn emails attached to the filing, Russell claimed he was being cooperative and has been trying to get assurances from Luminar that any personal data from his devices will be protected. \n“The company declined, so we will follow the court-established process for data handling protections instead,” Leonard Shulman, an attorney for Russell, told TechCrunch in a statement.\nThe emergency filing is one of the first major twists in a fast-moving bankruptcy case that involves Luminar trying to sell the two main parts of its business. The company is seeking court approval of an already reached agreement to sell its semiconductor subsidiary to a company called Quantum Computing, Inc., and has established a January 9 deadline for bids on its lidar division.\nRussell, through his new venture Russell AI Labs, tried to buy Luminar before the Chapter 11 filing and has expressed plans to make a bid in the bankruptcy process. “As it relates to Luminar, our focus remains on what matters: Russell AI Labs’ bid to rebuild the company and bring value to its stakeholders,” Shulman said.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nLawyers for Luminar said in the filing that it began hunting for information from Russell in May, just after he abruptly resigned following a “code of business conduct and ethics inquiry” performed by the board’s audit committee. The company was evaluating whether it may have potential legal claims against him “relating to the Audit Committee inquiry and personal loans taken by Mr. Russell,” according to the filing. But Luminar said those efforts were unsuccessful and Russell was not cooperative.\nOn November 12, Luminar’s board of directors established a Special Investigation Committee and hired law firm Weil, Gotshal & Manges to further investigate “certain acts, omissions, transactions and potential claims and causes of action involving or related to certain current and former directors and officers of Luminar.” \nOne month later, just before the bankruptcy, lawyers for Weil contacted the law firm of McDermott Will & Schulte, which had previously represented Russell. The Weil lawyers asked about collecting Russell’s Luminar-provided laptop and desktop computers, along with his company-issued phone and a digital copy (or “image”) of his personal phone. \nThe lawyers from Weil spent a week trying to confirm whether McDermott would represent Russell in the matter of the Special Investigation Committee, only to find out on December 19 that it would not. The Weil lawyers tried contacting Russell directly instead. \nRussell responded for the first time on Christmas Eve, according to the filing. He eventually authorized McDermott to turn over the computers (which the firm had kept in its possession since his resignation), but the founder repeatedly asked for guarantees that Luminar’s lawyers would not search through personal data on his phones, emails attached to the emergency filing show. \n“I have offered direct cooperation as well as prompt action, even through the holidays – but if this singular basic protection cannot be confirmed, I am advised further deliberations on this matter will not be productive,” Russell wrote in an email on New Year’s Eve.\nRepresentatives for Luminar arranged to have a forensic examiner appear at Russell’s mansion in Florida on New Year’s Day. But the technician was turned away by Russell’s security team, which a lawyer for Luminar called “unacceptable.” \nRussell claimed the technician was sent to his home “unannounced” on the holiday morning “when I was asleep,” and reiterated his desire to protect the privacy of his personal data. A lawyer for Luminar responded that they had “repeatedly confirmed that we have no intention of looking at any documents beyond those that are Luminar-related.” Russell replied on January 2 that “[a]ny characterization that I have been uncooperative is wholly inaccurate” and accused the lawyers of “word gymnastics.” \nLuminar’s lawyers tried instead to subpoena this information from Russell but claim their process servers were similarly turned away by his security team. They also claim those security team members lied about Russell’s presence at his Florida residence. \n“Can we try to serve Austin again today? We’re going to need someone dogged. He is going to evade service as long as possible,” one of the Weil lawyers wrote in an email on New Year’s Eve. “In fact, he was home when your person tried last time and the guard simply lied for him.”\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n"
  },
  {
    "title": "The 2026 BMW iX3 voice assistant will be powered by Alexa+",
    "url": "https://techcrunch.com/2026/01/05/the-2026-bmw-ix3-voice-assistant-will-be-powered-by-alexa/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/bmw-amazon.jpeg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T17:00:00.000Z",
    "description": "Image Credits:Amazon\t \t \t\t\t\t\t\t9:00 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t  \t\t\t BMW is finally getting the next-generation Alexa voice assistant and it’s coming with a generative AI upgrade. Amazon said Monday...",
    "body": "\n\t\n\t\tImage Credits:Amazon\t\n\t\n\t\t\t\t\t\t9:00 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nBMW is finally getting the next-generation Alexa voice assistant and it’s coming with a generative AI upgrade. \nAmazon said Monday that the 2026 BMW iX3 will be equipped with Alexa+, the same generative AI tech that launched in millions of the e-commerce giant’s smart devices last year. This will be the first vehicle to come with Amazon’s next-generation voice assistant, the companies announced during the 2026 Consumer Electronics Show in Las Vegas.\nThe launch is one part of Amazon’s effort to bring its LLM-powered voice and digital assistant to every device — whether handheld or in the driver’s seat — touched by consumers. Alexa+ is already in more than 600 million devices. And automotive is next on the list.\nBringing a custom version of Alexa+ into the BMW iX3 will be an important test for Amazon. Automakers have struggled for years to bring a voice assistant inside vehicles that can handle complex functions and requests that don’t end with the driver yelling in frustration. Efforts to develop natural language processing — a form of AI that lets computers understand and respond to human speech — have been in the works for more than a decade. And while progress has been made, these systems can still be easily stumped by humans.\nBMW and Amazon’s Alexa+ partnership has been three years in the making.\nBMW announced in 2022 that Amazon Alexa would be the foundation of its next-generation voice assistant. This meant BMW wouldn’t just embed Alexa into its vehicles but would also use Amazon’s technology platform known as Alexa Custom Assistant to build its own custom version. That timeline was extended as Amazon worked on an automotive version of Alexa+, an overhauled voice assistant developed and powered by large language models that promises to deliver seamless and natural conversations, like talking to a human.\nAlexa+ was built using Amazon Bedrock, a service that lets AWS customers build apps using generative AI models from Amazon and other third-party partners. Customers, like BMW, can then customize the app with their proprietary data.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nThe end result, according to Amazon, is a voice assistant that can break down complex requests, reason through steps, and take actions across different services. For instance, Amazon says users can start a conversation with their Alexa+-enabled Echo speaker in the home and continue it in their BMW. Once in the car, the user can make requests through the Alexa+ assistant that might normally require opening up different apps, like music, navigation, and a home security system.\n\n\t\t\t\n\tTopics\n\n\n\n\t\tKirsten Korosec is a reporter and editor who has covered the future of transportation from EVs and autonomous vehicles to urban air mobility and in-car tech for more than a decade. She is currently the transportation editor at TechCrunch and co-host of TechCrunch’s Equity podcast. She is also co-founder and co-host of the podcast, “The Autonocast.” She previously wrote for Fortune, The Verge, Bloomberg, MIT Technology Review and CBS Interactive.\nYou can contact or verify outreach from Kirsten by emailing kirsten.korosec@techcrunch.com or via encrypted message at kkorosec.07 on Signal.\t\n\t\n\t\tView Bio \n\t\n\n\t\t"
  },
  {
    "title": "Amazon’s AI assistant comes to the web with Alexa.com",
    "url": "https://techcrunch.com/2026/01/05/alexa-without-an-echo-amazons-ai-chatbot-comes-to-the-web-and-a-revamped-alexa-app/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Alexa.com_.png?w=150",
    "tag": "Tech",
    "date": "2026-01-05T15:00:00.000Z",
    "description": "Amazon’s AI-powered overhaul of its digital assistant, now known as Alexa+, is coming to the web. On Monday, at the start of CES 2026 in Las Vegas, the company announced the official launch of a new website,...",
    "body": "\nAmazon’s AI-powered overhaul of its digital assistant, now known as Alexa+, is coming to the web. On Monday, at the start of CES 2026 in Las Vegas, the company announced the official launch of a new website, Alexa.com, which is now rolling out to all Alexa+ Early Access customers. The site will allow customers to use Alexa+ online, much as you can do today with other AI chatbots such as ChatGPT or Google’s Gemini.\nWhile Alexa-powered devices, including Amazon’s Echo smart speakers and screens, have a well-established footprint with over 600 million devices sold worldwide, Amazon believes that for its AI assistant to be competitive, it will need to be everywhere — not just in the home, but also on the phone and on the web.\nPlus, the expansion could later give anyone a way to interact with Alexa+, even if they don’t have a device in their home.\nRelated to this expansion, Amazon is updating its Alexa mobile app, which will now offer a more “agent-forward” experience. Or, in other words, it’s putting a chatbot-style interface on the app’s homepage, making it seem more like a typical AI chatbot. (While you could chat with Alexa before in the app, the focus is now on the chatting — while the other features take a back seat.)\nImage Credits:Screenshot of the new Alexa app\nOn the Alexa.com website, customers can use Alexa+ for common tasks — for instance, exploring complex topics, creating content, and making trip itineraries. However, Amazon aims to differentiate its assistant from others by focusing on families and their needs in the home. That includes controlling smart devices, as you already could with the original Alexa, as well as doing things like updating the family’s calendar or to-do list, making dinner reservations, adding grocery items you need to your Amazon Fresh or Whole Foods cart, finding recipes and saving them to a library, or even planning the family movie night with personalized recommendations.\nMore recently, Amazon has been integrating more services with Alexa+, including the addition of Angi, Expedia, Square, and Yelp, which will join existing apps like Fodor’s, OpenTable, Suno, Ticketmaster, Thumbtack, and Uber.\nThe Alexa.com website features a navigation sidebar for quicker access to your most-used Alexa features, so you can pick up where you left off on tasks like setting the thermostat, checking your calendar for appointments, reviewing shopping lists, and more. \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nImage Credits:Amazon\nIn addition, Amazon aims to convince customers to share their personal documents, emails, and calendar access with Alexa+, so its AI can become a sort of hub to manage the goings-on at home, from kids’ school holidays and soccer schedules to doctor’s appointments and other things families need to remember — like when the dog got its last rabies shot, or what day the neighbor’s backyard BBQ is taking place. \nThis is an area where Amazon will need to stretch, as it doesn’t have its own productivity suite or the wealth of personal data that rivals like Google already have for their own customers. Instead, Amazon has been relying on tools to forward and upload files to Alexa+ for its AI to keep track of. That, too, will now be a feature available on Alexa.com, and the information you share can be displayed on the Echo Show’s screen, where it can also be managed.  \nThis ability to manage a family’s personal data could be Alexa’s biggest selling point, if it gets it right.\n“Seventy-six percent of what customers are using Alexa+ for no other AI can do,” says Daniel Rausch, VP of Alexa and Echo at Amazon, in an interview with TechCrunch. “And I think that’s a really interesting statistic about Alexa+ for two reasons. \nHe continues, “One, because customers count on Alexa to do unique things. You know, you can send a photograph of an old family recipe to Alexa and then talk through the recipe as you’re cooking it in your kitchen, substitute ingredients for what you have around the home, and get the job all the way done.”\nBut he notes, another 24% are using Alexa to do things other AIs can do — that could indicate they’re shifting more of their AI usage to Alexa+.\nImage Credits:Amazon\nAlexa.com will initially only be available to Early Access customers who sign in with their Amazon account. Amazon has been steadily rolling out Early Access since its debut of Alexa+ early last year.\nRausch tells us that tens of millions of consumers now have access to Alexa+, and they’re having two to three times more conversations with Alexa+ than they did with the original Alexa assistant. Specifically, they’re shopping three times more with Alexa+ and are using recipes five times more than before, he says. Heavy users of smart home devices also use Alexa+ 50% more for smart home control, compared with the original Alexa.\nHowever, across social media and online forums, there are complaints about Alexa+’s misfires and mistakes. But Rausch believes the complaints are overrepresented online. He says that the number of people opting out of the Alexa+ experience after trying it is in the low single digits, on average, or “effectively … almost none.”\n“Ninety-seven percent of Alexa devices support Alexa+, and we see now in adoption from customers that they’re using Alexa across all those many years and many generations of devices,” Rausch adds. “We support all of Alexa’s original capabilities, the tens of thousands of services and devices that Alexa was integrated with already are carried forward to the Alexa+ experience.” \n"
  }
]