[
  {
    "title": "Meta pauses international expansion of its Ray-Ban Display glasses",
    "url": "https://techcrunch.com/2026/01/06/meta-pauses-international-expansion-of-its-ray-ban-display-glasses/",
    "image": "https://techcrunch.com/wp-content/uploads/2025/09/Meta-Ray-Ban-Display-Outdoor-Lifestyle-1.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T15:04:49.000Z",
    "description": "\nIn Brief\nPosted:\n7:04 AM PST · January 6, 2026\n\nImage Credits:Meta\n\n\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\n\nMeta is pausing its plans to sell its R",
    "body": "\nIn Brief\nPosted:\n7:04 AM PST · January 6, 2026\n\nImage Credits:Meta\n\n\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\n\nMeta is pausing its plans to sell its Ray-Ban Display glasses outside the U.S. due to “unprecedented demand and limited supply,” the company said on Tuesday. Meta had originally planned to launch the glasses in France, Italy, Canada, and the U.K. in early 2026.\n“Since launching last fall, we’ve seen an overwhelming amount of interest, and as a result, product waitlists now extend well into 2026,” the company said. “Because of this unprecedented demand and limited inventory, we’ve decided to pause our planned international expansion.”\nMeta says it will continue focusing on fulfilling U.S. orders while it re-evaluates its approach to international availability.\nUnveiled in September, the Meta Ray-Ban Display smart glasses are controlled by a wristband called the Meta Neural Band, which detects subtle hand gestures.\nAt CES this week in Las Vegas, Meta showed off new features coming to the glasses and the Neural Band. The glasses are getting a new teleprompter feature that gives users a portable way to deliver prepared remarks. Plus, users can now jot down messages using their finger on any surface while wearing Meta Neural Band and have those movements transcribed into digital messages.\nMeta is also expanding pedestrian navigation to Denver, Las Vegas, Portland, and Salt Lake City. \n\n\n\tTopics\n\n\n\t\n\t\tSubscribe for the industry’s biggest tech news\n\t\n\nLatest in Hardware\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Meta pauses international expansion of its Ray-Ban Display glasses",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2025/09/Meta-Ray-Ban-Display-Outdoor-Lifestyle-1.jpg?w=150"
      ],
      "datePublished": "2026-01-06T15:04:49.000Z",
      "dateModified": "2026-01-06T15:04:49.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Intel is building a handheld gaming platform including a dedicated chip",
    "url": "https://techcrunch.com/2026/01/06/intel-is-building-a-handheld-gaming-platform-including-a-dedicated-chip/",
    "image": "https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205047441.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T19:39:59.000Z",
    "description": "\nIn Brief\nPosted:\n11:39 AM PST · January 6, 2026\n\nImage Credits:Joan Cros/NurPhoto / Getty Images\n\n\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\n\nIntel is ",
    "body": "\nIn Brief\nPosted:\n11:39 AM PST · January 6, 2026\n\nImage Credits:Joan Cros/NurPhoto / Getty Images\n\n\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\n\nIntel is looking to double down on gaming hardware with a new chip and platform for portable gaming devices.\nThe platform will include hardware and software, Intel vice president and general manager of PC products, Daniel Rogers, announced at CES on Monday. It will be built off of the company’s Intel Core Series 3 processors, known as Panther Lake, which was announced last year and is now being rolled out in a variety of PCs.\nThis future platform includes a chip specifically for handheld gaming devices, according to reporting from IGN, which was confirmed by TechCrunch.\nThese Panther Lake chips are the company’s first built on its 18A manufacturing process which started production in 2025.\nIntel is no stranger to the gaming industry and has been building chips for gaming PCs since the 90s. The company leaned more heavily into gaming in 2022 with the release of its Intel Arc GPUs.\nEntering the handheld gaming space would be an interesting development, though, as the market is currently dominated by AMD. AMD just announced a new processor designed for gaming PCs, AMD Ryzen 7 9850X3D, in its CES keynote on Monday, in addition to new ray tracing and graphics technologies for gaming.\nRogers said Intel will share more details about its new products for handheld gaming devices later this year. \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nFollow along with all of TechCrunch’s coverage of the annual CES conference here.\n\n\n\tTopics\n\n\n\t\n\t\tSubscribe for the industry’s biggest tech news\n\t\n\nLatest in Gaming\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Intel is building a handheld gaming platform including a dedicated chip",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205047441.jpg?w=150"
      ],
      "datePublished": "2026-01-06T19:39:59.000Z",
      "dateModified": "2026-01-06T19:39:59.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "LMArena lands $1.7B valuation four months after launching its product",
    "url": "https://techcrunch.com/2026/01/06/lmarena-lands-1-7b-valuation-four-months-after-launching-its-product/",
    "image": "https://techcrunch.com/wp-content/uploads/2023/10/GettyImages-1486380350.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T19:35:55.000Z",
    "description": "\nLMArena, a startup that originally launched as UC Berkeley research project in 2023, announced on Tuesday that it raised an $150 million Series A at ",
    "body": "\nLMArena, a startup that originally launched as UC Berkeley research project in 2023, announced on Tuesday that it raised an $150 million Series A at a post-money valuation of $1.7 billion. The round was led by Felicis and the university’s fund UC Investments.\nThe startup bolted out of the gate as a commercial venture with a $100 million seed round in May at a $600 million valuation. This new rounds means it raised $250 million in about seven months.\nLMArena is best known for its crowdsourced AI model performance leaderboards. Its consumer website lets a user type a prompt that it sends to two models, with the user then choosing which model did a better job. Those results, which now span more than 5 million monthly users across 150 countries and 60 million conversations a month, the company says, fuel the leaderboards. It ranks various models on a variety of tasks including text, web development, vision, text-to-image, and other criteria.\nThe models it tests include various flavors of OpenAI GPT, Google Gemini, Anthropic Claude, and Grok, as well as ones that are geared toward specialties like image generation, text to image, or reasoning.\nThe company began as Chatbot Arena, an open research project built by UC Berkeley researchers Anastasios Angelopoulos and Wei-Lin Chiang and was originally funded through grants and donations. \nLMArena’s leaderboards became something of an obsession among model makers. When LMArena started pursuing revenue, it partnered with select model companies such as OpenAI, Google, and Anthropic to make their flagship models available for its community to evaluate. In April, a group of competitors published a paper alleging that this helped those model makers game the startup’s benchmarks, an allegation LMArena has vehemently denied.\nIn September, it publicly launched a commercial service, AI Evaluations, in which enterprises, model labs, and developers can hire the company to perform model evaluations through its community. This gave LMArena an annualized “consumption rate” — as the company describes its annual recurring revenue (ARR) — of $30 million as of December, less than four months after launch\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nThat trajectory, and the startup’s popularity, were enough for VCs to pile in for the Series A, which included participation from Andreessen Horowitz, The House Fund, LDVP, Kleiner Perkins, Lightspeed Venture Partners and Laude Ventures.\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "LMArena lands $1.7B valuation four months after launching its product",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2023/10/GettyImages-1486380350.jpg?w=150"
      ],
      "datePublished": "2026-01-06T19:35:55.000Z",
      "dateModified": "2026-01-06T19:35:55.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Threads is developing in-message games",
    "url": "https://techcrunch.com/2026/01/06/threads-is-developing-in-message-games/",
    "image": "https://techcrunch.com/wp-content/uploads/2024/12/instagram-threads-GettyImages-2159215889.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T17:39:13.000Z",
    "description": "\n\t\t\t\t\t\t9:39 AM PST · January 6, 2026\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\nThreads is exploring games in chats, beginning with a basketball game. A spokesperson for Meta con",
    "body": "\n\t\t\t\t\t\t9:39 AM PST · January 6, 2026\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\nThreads is exploring games in chats, beginning with a basketball game. A spokesperson for Meta confirmed to TechCrunch that the company is internally prototyping the game and that it’s not available to the public.\nThe game was first spotted by reverse engineer Alessandro Paluzzi, who often finds unreleased features while they’re still under development. Paluzzi shared a screenshot of the basketball game, which appears to let users virtually shoot hoops by swiping their finger. The idea behind the game is likely to allow friends to compete to see who can score the most baskets, similar to other mobile basketball games.\n\nLaunching in-message games would give Threads an edge over competitors like X and Bluesky, which don’t offer built-in games. It could also even help Threads compete with Apple’s Messages, which supports games via third-party apps like GamePigeon.\nAs with any internal prototype, it’s unknown when or if Meta plans to officially roll out games in Threads messages.\nIt’s also worth noting this isn’t the first time Meta has explored in-message gaming, as Instagram launched a hidden emoji game in DMs last year. The goal of the game is to use your finger to move the paddle at the bottom of the screen to keep an emoji of your choice afloat and continuously bouncing. If you let the emoji fall, you lose. The idea is to compete with the other person in the chat to achieve the highest score.\nThe internal prototype comes as Meta continues to build out Threads with new features to take on its competitors. For example, the platform recently expanded its Communities feature with more topics, likely aiming to draw users away from Reddit and X. Plus, it added a “disappearing posts” feature that lets users share their thoughts and engage in conversations that are automatically archived after 24 hours.\nWhile Threads boasts 400 million monthly users, it still has quite a way to go to catch up with X in the U.S., according to data from Pew Research Center’s report released a few weeks ago. The report says 21% of U.S. adults said they have used X, compared with only 8% who have used Threads, and 4% who have used Bluesky.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\n\n\t\t\t\n\tTopics\n\n\n\n\t\tAisha is a consumer news reporter at TechCrunch. Prior to joining the publication in 2021, she was a telecom reporter at MobileSyrup. Aisha holds an honours bachelor’s degree from University of Toronto and a master’s degree in journalism from Western University.\nYou can contact or verify outreach from Aisha by emailing aisha@techcrunch.com or via encrypted message at aisha_malik.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n\n\t\t",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Threads is developing in-message games",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2024/12/instagram-threads-GettyImages-2159215889.jpg?w=150"
      ],
      "datePublished": "2026-01-06T17:39:13.000Z",
      "dateModified": "2026-01-06T17:39:13.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Narwal adds AI to its vacuum cleaners to monitor pets and find jewelry",
    "url": "https://techcrunch.com/2026/01/05/narwal-adds-ai-to-its-vacuum-cleaners-to-monitor-pets-and-find-jewelry/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/002.jpeg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T07:18:00.000Z",
    "description": "\n\t\n\t\tImage Credits:Narwal\t\n\t\n\t\t\t\t\t\t11:18 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nRobot vacuum maker Narwal unveiled its new set of smart vacuum clean",
    "body": "\n\t\n\t\tImage Credits:Narwal\t\n\t\n\t\t\t\t\t\t11:18 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nRobot vacuum maker Narwal unveiled its new set of smart vacuum cleaners at the Consumer Electronics Show with AI-powered features such as monitoring pets, finding valuable objects, and notifying users about misplaced toys.\nThe company said that its new flagship Flow 2 robot vacuum has a rounded design and easy-lift tanks for better cleaning. The device uses two 1080p RGB cameras with a 136-degree field of view to map out the area and recognize different kinds of objects using AI models.\nNarwal said that through this tech stack, the vacuum cleaner has the ability to identify an unlimited number of objects. The device first tries to identify an object locally, but in case there are no matches, it sends the data to the cloud for further processing. \nImage Credits:Narwal\nThe Flow 2 has three key modes called pet care mode, baby care mode, and AI floor tag mode. With pet care mode, you can define zones where pets usually rest or hang out to clean themselves. Plus, it can monitor pets and also check in on your pets via two-way audio (there is no guarantee they will listen to you, though). In the baby care mode, the vacuum switches to quiet mode near the crib and notifies you of misplaced toys. In the AI floor tag mode, the vacuum recognizes valuable items like jewelry, avoids them, and alerts you.\nNarwal said that its newest vacuum cleaner has four cleaning modes that can identify different types of dirt. The device can also return to its base to wash the mop and then re-mop a certain area if it is dirty. The company noted that the Flow 2’s design allows for a higher hot water washing temperature for better cleaning.\nImage Credits:Narwal\nAlong with the Flow 2, the company also showed off a handheld vacuum called the U50 that weighs 1.41 kg (3.1 lbs) and has UV-C sterilization along with heat treatment for allergen removal. The company also demoed an unnamed cordless vacuum with a slim design, 360-degree swivel, and up to 50 minutes of run time. The cordless vacuum also has an auto-empty station that can support up to 60 days of dust disposal.\n\n\t\t\t\n\tTopics\n\n\n\n\t\tIvan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web.\r\nYou can contact or verify outreach from Ivan by emailing im@ivanmehta.com or via encrypted message at ivan.42 on Signal.\t\n\t\n\t\tView Bio \n\t\n\n\t\t",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Narwal adds AI to its vacuum cleaners to monitor pets and find jewelry",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/002.jpeg?w=150"
      ],
      "datePublished": "2026-01-06T07:18:00.000Z",
      "dateModified": "2026-01-06T07:18:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Amazon’s Ring doorbells get fire alerts, an app store, and new sensors",
    "url": "https://techcrunch.com/2026/01/06/amazons-ring-doorbells-get-fire-alerts-an-app-store-and-new-sensors/",
    "image": "https://techcrunch.com/wp-content/uploads/2025/12/20250714_lifestyle_wireddoorbellprogen3_sn_frontdoor_RGB.jpeg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T14:00:00.000Z",
    "description": "\nAmazon’s augmenting its Ring smart doorbells with new features, including new fire alerts, an app store, and a new set of Ring Sensors.\nAnnounced at ",
    "body": "\nAmazon’s augmenting its Ring smart doorbells with new features, including new fire alerts, an app store, and a new set of Ring Sensors.\nAnnounced at CES 2026 in Las Vegas, the company said the new Ring Sensors can detect motion, openings, glass breakage, and smoke, and can also monitor carbon monoxide levels, leaks, temperature changes, and air quality. They also let you control lighting and appliances that are connected to your smart home network.\nAmazon is also adding a new app store to the Ring app to allow users to use its cameras with third-party apps. The store is currently available in the U.S. only, and users will be able to browse a selection of apps in the coming weeks. While the company didn’t specify what kind of apps the store would feature, it said these apps would focus on small business operations and everyday needs around the house.\nAnd as fires grow more common in drought-affected areas, the company has teamed up with the fire monitoring app Watch Duty to show real-time updates and early warnings in the Ring app’s Neighbors section. Users can also share live updates to this section using their Ring cameras.\nNotably, the new devices being launched support Amazon’s Sidewalk shared network feature, which creates a mesh network between the company’s Echo and Ring devices by sharing a small portion of your bandwidth, enabling devices to work even when they aren’t in wireless range of your Wi-Fi router.\nAmazon is also adding a new AI-based feature to Ring cameras that learns the everyday patterns of a property and alerts users if anything unusual occurs. Dubbed “AI Unusual Event Alerts,” the company said the feature enables the Ring camera to surface specific warnings when it detects a person, based on location, actions, and clothing.\nFor people who subscribe to Amazon’s Virtual Security Guard services, these warnings could trigger intervention automatically. In the last month, the company has rolled out features that use video recognition to trigger Alexa responses and send you personalized notifications based on a database of 50 faces.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nThe company has also launched a new Ring Car Alarm with in-built GPS for vehicle monitoring.\n\n\n\t\tIvan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web.\r\nYou can contact or verify outreach from Ivan by emailing im@ivanmehta.com or via encrypted message at ivan.42 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Amazon’s Ring doorbells get fire alerts, an app store, and new sensors",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2025/12/20250714_lifestyle_wireddoorbellprogen3_sn_frontdoor_RGB.jpeg?w=150"
      ],
      "datePublished": "2026-01-06T14:00:00.000Z",
      "dateModified": "2026-01-06T14:00:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "AMD unveils new AI PC processors for general use and gaming at CES",
    "url": "https://techcrunch.com/2026/01/05/amd-unveils-new-ai-pc-processors-for-general-use-and-gaming-at-ces/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Ryzen-AI-Blog-1200x675-1.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T03:30:00.000Z",
    "description": "\nAMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone.\nAs part of that promis",
    "body": "\nAMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone.\nAs part of that promise, AMD announced a new line of AI processors as the company thinks AI-powered personal computers are the way of the future.\nThe semiconductor giant revealed the Ryzen AI 400 Series processor, the latest version of its AI-powered PC chips, at the yearly CES conference on Monday. The company says the latest version of its Ryzen processor series allows for 1.3x faster multitasking than its competitors and are 1.7x times faster at content creation.\nThese new chips feature 12 CPU Cores, individual processing units inside a core processor, and 24 threads, independent streams of instruction.\nThis is an upgrade to the Ryzen AI 300 Series processor that was announced in 2024. AMD started producing the Ryzen processor series in 2017.\nRahul Tikoo, senior vice president and general manager of AMD’s client business, said AMD has expanded to over 250 AI PC platforms on the company’s recent press briefing. That represents a growth 2x over the last year, he added.\n“In the years ahead, AI is going to be a multi-layered fabric that gets woven into every level of computing at the personal layer,” Tikoo said. “Our AI PCs and devices will transform how we work, how we play, how we create and how we connect with each other.”\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAMD also announced the release of the AMD Ryzen 7 9850X3D, the latest version of its gaming-focused processor.\n“No matter who you are and how you use technology on a daily basis, AI is reshaping everyday computing,” Tikoo said. “You have thousands of interactions with your PC every day. AI is able to understand, learn context, bring automation, provide deep reasoning and personal customization to every individual.”\nPCs that include either the Ryzen AI 400 Series processor or the AMD Ryzen 7 9850X3D processor become available in the first quarter of 2026.\nThe company also announced the latest version of its Redstone ray tracing technology, which simulates physical behavior of light, which allows for better video game graphics without a performance or speed lag.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here. \n\n\n\n\n\n\t\tBecca is a senior writer at TechCrunch that covers venture capital trends and startups. She previously covered the same beat for Forbes and the Venture Capital Journal.\r\nYou can contact or verify outreach from Becca by emailing rebecca.szkutak@techcrunch.com.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "AMD unveils new AI PC processors for general use and gaming at CES",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/Ryzen-AI-Blog-1200x675-1.jpg?w=150"
      ],
      "datePublished": "2026-01-06T03:30:00.000Z",
      "dateModified": "2026-01-06T03:30:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Nvidia wants to be the Android of generalist robotics ",
    "url": "https://techcrunch.com/2026/01/05/nvidia-wants-to-be-the-android-of-generalist-robotics/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-05-at-5.03.42-PM.png?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:00:00.000Z",
    "description": "\nNvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to ",
    "body": "\nNvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to become the default platform for generalist robotics, much as Android became the operating system for smartphones. \nNvidia’s move into robotics reflects a broader industry shift as AI moves off the cloud and into machines that can learn how to think in the physical world, enabled by cheaper sensors, advanced simulation, and AI models that increasingly can generalize across tasks. \nNvidia revealed details on Monday about its full-stack ecosystem for physical AI, including new open foundation models that allow robots to reason, plan, and adapt across many tasks and diverse environments, moving beyond narrow task-specific bots, all of which are available on Hugging Face. \nThose models include: Cosmos Transfer 2.5 and Cosmos Predict 2.5, two world models for synthetic data generation and robot policy evaluation in simulation; Cosmos Reason 2, a reasoning vision language model (VLM) that allows AI systems to see, understand, and act in the physical world; and Isaac GR00T N1.6, its next-gen vision language action (VLA) model purpose-built for humanoid robots. GR00T relies on Cosmos Reason as its brain, and it unlocks whole-body control for humanoids so they can move and handle objects simultaneously. \nNvidia also introduced Isaac Lab-Arena at CES, an open source simulation framework hosted on GitHub that serves as another component of the company’s physical AI platform, enabling safe virtual testing of robotic capabilities.\nThe platform promises to address a critical industry challenge: As robots learn increasingly complex tasks, from precise object handling to cable installation, validating these abilities in physical environments can be costly, slow, and risky. Isaac Lab-Arena tackles this by consolidating resources, task scenarios, training tools, and established benchmarks like Libero, RoboCasa, and RoboTwin, creating a unified standard where the industry previously lacked one.\nSupporting the ecosystem is Nvidia OSMO, an open source command center that serves as connective infrastructure that integrates the entire workflow from data generation through training across both desktop and cloud environments. \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAnd to help power it all, there’s the new Blackwell-powered Jetson T4000 graphics card, the newest member of the Thor family. Nvidia is pitching it as a cost-effective on-device compute upgrade that delivers 1200 teraflops of AI compute and 64 gigabytes of memory while running efficiently at 40 to 70 watts. \nNvidia is also deepening its partnership with Hugging Face to let more people experiment with robot training without needing expensive hardware or specialized knowledge. The collaboration integrates Nvidia’s Isaac and GR00T technologies into Hugging Face’s LeRobot framework, connecting Nvidia’s 2 million robotics developers with Hugging Face’s 13 million AI builders. The developer platform’s open source Reachy 2 humanoid now works directly with Nvidia’s Jetson Thor chip, letting developers experiment with different AI models without being locked into proprietary systems.  \nThe bigger picture here is that Nvidia is trying to make robotics development more accessible, and it wants to be the underlying hardware and software vendor powering it, much like Android is the default for smartphone makers.\nThere are early signs that Nvidia’s strategy is working. Robotics is the fastest growing category on Hugging Face, with Nvidia’s models leading downloads. Meanwhile robotics companies, from Boston Dynamics and Caterpillar to Franka Robots and NEURA Robotics, are already using Nvidia’s tech.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here.\n\n\n\n\t\tRebecca Bellan is a senior reporter at TechCrunch where she covers the business, policy, and emerging trends shaping artificial intelligence. Her work has also appeared in Forbes, Bloomberg, The Atlantic, The Daily Beast, and other publications. \r\nYou can contact or verify outreach from Rebecca by emailing rebecca.bellan@techcrunch.com or via encrypted message at rebeccabellan.491 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Nvidia wants to be the Android of generalist robotics ",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-05-at-5.03.42-PM.png?w=150"
      ],
      "datePublished": "2026-01-05T23:00:00.000Z",
      "dateModified": "2026-01-05T23:00:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Insight Partners sued by former vice president Kate Lowry",
    "url": "https://techcrunch.com/2026/01/05/insight-partners-sued-by-former-vice-president-kate-lowry/",
    "image": "https://techcrunch.com/wp-content/uploads/2023/11/gavel-messy-legal.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:30:00.000Z",
    "description": "\nKate Lowry, a former vice president at Insight Partners, is suing the firm, alleging disability discrimination, gender discrimination, and wrongful t",
    "body": "\nKate Lowry, a former vice president at Insight Partners, is suing the firm, alleging disability discrimination, gender discrimination, and wrongful termination, according to a suit filed on December 30 in San Mateo County, California, and seen by TechCrunch. Insight Partners did not immediately respond to TechCrunch’s request for comment.\nLowry told TechCrunch she filed the suit because she believes “too many powerful, wealthy people in venture act like it’s OK to break the law and systemically underpay and abuse their employees.”\n“It’s an oppressive system that reflect[s] broader trends in society that use fear, intimidation, and power to silence and isolate truth. I’m trying to change that.”\nLowry began working at Insight Partners in 2022, after previously working for Meta, McKinsey & Company, and an early-stage startup. The suit alleges that, upon being hired, she was assigned to a different supervisor than the person mentioned during her interview.  \nShe alleges in the suit that she was told by her new supervisor, who was a woman, to be “online all the time, including PTO, holidays, and weekends,” and to respond between “6 a.m. and 11 p.m. daily.”  \nLowry says in the suit that this first supervisor “berated, hazed, and antagonized” her, spoke openly about a hazing that would be “longer and more intense” than what she put other male reports through.  \nSome comments the supervisor allegedly made, according to the suit, include “you are incompetent, shut up and take notes” and “you need to obey me like a dog; do whatever I say whenever I say it, without speaking.”  Lowry also alleges that her supervisor assigned her “redundant tasks” and restricted her ability to participate in calls, while allowing less experienced male colleagues to do so. Lowry, instead, she alleges, was relegated to “administrative tasks such as note-taking and cataloging.”  \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nLowry said she became “increasingly ill” because of the work environment and that her physician advised a medical leave of absence, which she was granted and took from February to July 2023.  \nWhen she returned to work, she was placed on a new team and, the suit alleges, was told by the head of human resources that “if the new team did not like her, she would be fired.”  \nIn September 2023, Lowry said she got a concussion and took another medical leave and returned to work near the end of 2024. Due to some departures, she was placed under the supervision of a new person, where Lowry said her poor treatment continued. She also alleges that in 2024, her compensation was about 30% below the market. \nBy April 2025, she alleges she was told her compensation would be cut. In May of 2025, through her attorneys, Lowry sent a letter to Insight regarding her alleged treatment by the company. A week later, the firm terminated her employment, the suit states.  \nThe lawsuit is reminiscent of Ellen Pao’s suit against Kleiner Perkins back in 2012, in which she alleged discrimination and retaliation. That suit offered what was, at the time, a rare glimpse into how women partners felt they were treated in venture capital. Though Pao lost that suit, it sent waves through the industry, and other women went on to sue major tech companies.  \n\n\n\t\tDominic-Madori Davis is a senior venture capital and startup reporter at TechCrunch. She is based in New York City.\r\nYou can contact or verify outreach from Dominic by emailing dominic.davis@techcrunch.com or via encrypted message at +1 646 831-7565 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Insight Partners sued by former vice president Kate Lowry",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2023/11/gavel-messy-legal.jpg?w=150"
      ],
      "datePublished": "2026-01-05T23:30:00.000Z",
      "dateModified": "2026-01-05T23:30:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "This is Uber’s new robotaxi from Lucid and Nuro",
    "url": "https://techcrunch.com/2026/01/05/this-is-ubers-new-robotaxi-from-lucid-and-nuro/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/uber-lucid-gravity.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:00:04.000Z",
    "description": "\nUber, Lucid Motors, and Nuro have revealed the production-intent version of their collaborative robotaxi at the 2026 Consumer Electronics Show, and T",
    "body": "\nUber, Lucid Motors, and Nuro have revealed the production-intent version of their collaborative robotaxi at the 2026 Consumer Electronics Show, and TechCrunch got a sneak peek ahead of the reveal.\nIt’s a vehicle that’s been in the works for more than half a year now, part of a deal that saw Uber invest $300 million into Lucid and commit to buying 20,000 of the company’s EVs. On Monday, the companies said the robotaxi is already being tested on public roads ahead of a planned commercial service launching in the San Francisco Bay Area later this year.\nBased on the Lucid Gravity SUV, the robotaxi has high-resolution cameras, solid state lidar sensors, and radars integrated into the body and the roof-mounted “halo.” The autonomy package is powered by Nvidia’s Drive AGX Thor computer. That halo also has integrated LED lights that will help riders identify their vehicle (similar to how Waymo’s Jaguar I-Pace SUVs work).\nCrucially, all of this extra tech is added to the Gravity as it’s being built at Lucid Motors’ Casa Grande, Arizona factory, saving the companies some time and money. By comparison, Waymo currently has to take apart the I-Pace SUVs it receives from Jaguar and integrate the autonomous tech as it puts them back together. (Future Waymo vehicles are planned to be more purpose-built.)\nImage Credits:Sean O'Kane\nThe vehicle unveiled on Monday is a more polished-up version of the test version that the three companies have spent the last seven months showing off in press photos. The newest element revealed at CES has to do with how users will interface with the Uber-Lucid-Nuro robotaxi. That includes a small screen on the halo meant to greet riders and a ride interface inside the cabin.\nAnyone who has ridden in a Waymo will find this UI experience familiar. The rear passenger screen shows an isometric graphical view of the robotaxi moving through city streets, with representations of nearby cars and pedestrians. \nThe companies did not have an interactive version of the software — which is being created by Uber — ready to test out just yet. But it has been built to show the standard information like estimated drop-off time, how much ride time is remaining, and climate and music controls. There are also buttons to reach rider support and to tell the robotaxi to pull over.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nThe front passenger screen shows a lot of the same information, just on a larger central touchscreen display. In the demonstration car on display at the Fontainebleau hotel, a lot of the same elements appeared on the Gravity’s sweeping 34-inch curved OLED display, which sits behind the steering wheel. \nUber chose to build this forthcoming “premium” robotaxi service around the Gravity, and at a high level it seems like a wise decision. The Gravity is immensely spacious inside, especially in the two-row configuration on display at the hotel. (Uber says a three-row version will be available, too.) \nThat said, the Gravity’s first full year came with struggles. Lucid fought with software issues as it ramped up production of the SUV, and the problems got bad enough that interim CEO Marc Winterhoff sent an email to owners in December apologizing for the “frustrations” they experienced. \nLucid has seemingly been able to bounce back from that, and on Monday announced that it doubled its 2024 production figures and reached new sales records. Time will tell if the robotaxi version has any of the same kinds of software struggles.\nUber, Lucid, and Nuro said Monday that once final validation is complete on the robotaxi later this year, true production versions will start rolling off Lucid’s factory lines in Arizona. The companies did not give a concrete timeline for that, though.\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "This is Uber’s new robotaxi from Lucid and Nuro",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/uber-lucid-gravity.jpg?w=150"
      ],
      "datePublished": "2026-01-05T23:00:04.000Z",
      "dateModified": "2026-01-05T23:00:04.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Nvidia launches Alpamayo, open AI models that allow autonomous vehicles to ‘think like a human’",
    "url": "https://techcrunch.com/2026/01/05/nvidia-launches-alpamayo-open-ai-models-that-allow-autonomous-vehicles-to-think-like-a-human/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Alpamayo-Image.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T21:52:22.000Z",
    "description": "\nAt CES 2026, Nvidia launched Alpamayo, a new family of open source AI models, simulation tools, and datasets for training physical robots and vehicle",
    "body": "\nAt CES 2026, Nvidia launched Alpamayo, a new family of open source AI models, simulation tools, and datasets for training physical robots and vehicles that are designed to help autonomous vehicles reason through complex driving situations.\n“The ChatGPT moment for physical AI is here – when machines begin to understand, reason, and act in the real world,” Nvidia CEO Jensen Huang said in a statement. “Alpamayo brings reasoning to autonomous vehicles, allowing them to think through rare scenarios, drive safely in complex environments, and explain their driving decisions.” \nAt the core of Nvidia’s new family is Alpamayo 1, a 10 billion-parameter chain-of-thought, reason-based vision language action (VLA) model that allows an AV to think more like a human so it can solve complex edge cases — like how to navigate a traffic light outage at a busy intersection — without previous experience. \n“It does this by breaking down problems into steps, reasoning through every possibility, and then selecting the safest path,” Ali Kani, Nvidia’s vice president of automotive, said Monday during a press briefing. \nOr as Huang put it during his keynote on Monday: “Not only does [Alpamayo] take sensor input and activate steering wheel, brakes, and acceleration, it also reasons about what action it’s about to take. It tells you what action it’s going to take, the reasons by which it came about that action. And then, of course, the trajectory.”\nAlpamayo 1’s underlying code is available on Hugging Face. Developers can fine-tune Alpamayo into smaller, faster versions for vehicle development, use it to train simpler driving systems, or build tools on top of it like auto-labeling systems that automatically tag video data or evaluators that check if a car made a smart decision. \n“They can also use Cosmos to generate synthetic data and then train and test their Alpamayo-based AV application on the combination of the real and synthetic dataset,” Kani said. Cosmos is Nvidia’s brand of generative world models, AI systems that create a representation of a physical environment so they can make predictions and take actions. \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAs part of the Alpamayo rollout, Nvidia is also releasing an open dataset with more than 1,700 hours of driving data collected across a range of geographies and conditions, covering rare and complex real-world scenarios. The company is additionally launching AlpaSim, an open source simulation framework for validating autonomous driving systems. Available on GitHub, AlpaSim is designed to recreate real-world driving conditions, from sensors to traffic, so developers can safely test systems at scale.\n\n\n\n\t\tRebecca Bellan is a senior reporter at TechCrunch where she covers the business, policy, and emerging trends shaping artificial intelligence. Her work has also appeared in Forbes, Bloomberg, The Atlantic, The Daily Beast, and other publications. \r\nYou can contact or verify outreach from Rebecca by emailing rebecca.bellan@techcrunch.com or via encrypted message at rebeccabellan.491 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Nvidia launches Alpamayo, open AI models that allow autonomous vehicles to ‘think like a human’",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/Alpamayo-Image.jpg?w=150"
      ],
      "datePublished": "2026-01-05T21:52:22.000Z",
      "dateModified": "2026-01-05T21:52:22.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Lego Smart Bricks introduce a new way to build — and they don’t require screens",
    "url": "https://techcrunch.com/2026/01/05/lego-smart-bricks-introduce-a-new-way-to-build-and-they-dont-require-screens/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/SMARTBrick_16x9.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T20:31:51.000Z",
    "description": "\n\t\n\t\tImage Credits:Lego\t\n\t\n\t\t\t\t\t\t12:31 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLego announced its new Smart Play system at CES 2026 on Monday, adding",
    "body": "\n\t\n\t\tImage Credits:Lego\t\n\t\n\t\t\t\t\t\t12:31 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLego announced its new Smart Play system at CES 2026 on Monday, adding interactive, responsive Legos to the famously analog franchise.\nThe Smart Play system includes a 2×4 brick, Smart Tag tiles, and Smart Minifigures. The Smart Bricks and Minifigures can sense nearby Smart Tags, which are 2×2 studless tiles with unique digital IDs that tell the Bricks and Minifigures how to act.\nIf the Smart Tag comes in a set for building a helicopter, for example, then the Smart Brick will light up and make propeller sounds that would help bring a helicopter to life. Its built-in accelerometer would make these lights and sounds more consistent with how you’re actually playing with the helicopter, since the Brick will be able to sense when the helicopter is zooming through the sky or turned upside down.\nImage Credits:LEGO\nThe Smart Bricks are powered by a patented ASIC chip, which is smaller than the size of a single Lego stud. The chip uses near-field magnetic positioning to recognize the Tags around it, as well as a miniature speaker, accelerometer, and LED array. Lego also developed a Bluetooth-based protocol called BrickNet, which allows multiple Smart Bricks to recognize each other and operate in tandem. The company claims that BrickNet is protected by enhanced encryption and privacy controls (all of which is necessary, but imagine a world where hacking into toys wasn’t a concern!).\nThere’s no setup required to pair the elements of the Smart Play system, making it easy for kids to get started — and parents will be pleased to note that there are no screens involved in the Smart system at all. However, Lego’s website says that there will be a Smart Tag for animating Lego toilets, so… there’s that.\nLego’s first two Smart Play sets — which are both Star Wars-themed — will launch on March 1, though preorders open on Friday. The “Luke’s Red Five X-wing” building set will retail for $69.99, while the larger “Throne Room Duel and A-wing” set will cost $159.99. These sets use the Smart Play system to animate characters like Luke Skywalker and Princess Leia, allowing them to interact with Smart Tags, which enable Lightsaber duels among other Star Wars-related capabilities.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\n\n\t\t\t\n\tTopics\n\n\n\n\t\tAmanda Silberling is a senior writer at TechCrunch covering the intersection of technology and culture. She has also written for publications like Polygon, MTV, the Kenyon Review, NPR, and Business Insider. She is the co-host of Wow If True, a podcast about internet culture, with science fiction author Isabel J. Kim. Prior to joining TechCrunch, she worked as a grassroots organizer, museum educator, and film festival coordinator. She holds a B.A. in English from the University of Pennsylvania and served as a Princeton in Asia Fellow in Laos.\nYou can contact or verify outreach from Amanda by emailing amanda@techcrunch.com or via encrypted message at @amanda.100 on Signal. \t\n\t\n\t\tView Bio \n\t\n\n\t\t",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Lego Smart Bricks introduce a new way to build — and they don’t require screens",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/SMARTBrick_16x9.jpg?w=150"
      ],
      "datePublished": "2026-01-05T20:31:51.000Z",
      "dateModified": "2026-01-05T20:31:51.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Boston Dynamics’ next-gen humanoid robot will have Google DeepMind DNA",
    "url": "https://techcrunch.com/2026/01/05/boston-dynamicss-next-gen-humanoid-robot-will-have-google-deepmind-dna/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/atlas-announcement.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T21:55:55.000Z",
    "description": "\nRobotics company Boston Dynamics has struck a partnership with Google’s AI research lab to speed up the development of its next-generation humanoid r",
    "body": "\nRobotics company Boston Dynamics has struck a partnership with Google’s AI research lab to speed up the development of its next-generation humanoid robot Atlas — and make it act more human around people.\nThe partnership, which was announced Monday during the Hyundai press conference at CES 2026, is centered on robotics research that will use Google DeepMind’s AI foundation models. Boston Dynamics’ humanoid robot Atlas will be the first test case, according to Carolina Parada, senior director of robotics at Google DeepMind.\n“We’re looking to integrate our cutting-edge AI foundation models with Boston Dynamics’ new Atlas robots, and we’ll aim to develop the world’s most advanced robot foundation model to fulfill the promise of true general-purpose human needs,” Parada said onstage.\nThe tie-up comes less than a year after the Google AI research lab announced new AI models called Gemini Robotics that are designed to allow robots to perceive, reason, use tools, and interact with humans. Gemini Robotics is based on a large-scale multimodal generative AI model, Gemini. At the time, Google DeepMind said the robotics AI model was trained to generalize behavior across a range of different robotics hardware.\nEnter Boston Dynamics, and its majority owner, Hyundai Motor Group. While accelerating research will be a central piece of this partnership, this has real-world scaling intent.\nBoston Dynamics already has products, like the quadruped Spot, that are in customers’ hands in more than 40 countries. Its warehouse robot Stretch has unloaded more than 20 million boxes globally since its launch in 2023, according to Hyundai. Now Boston Dynamics and Hyundai are preparing for the next generation, starting with the humanoid robot Atlas, which the company announced Monday is already in production and headed to the Hyundai factory in Savannah, Georgia\nA prototype of Atlas walked onstage during the press conference, showing off its ability to move. But as Alberto Rodriguez, director of Atlas behavior at Boston Dynamics, noted, making “Atlas into a product requires more than athletic performance for humanoids to really deliver on their promise. They have to be able to interact with people naturally.”\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nRodriguez and his counterparts at Boston Dynamics believe that recent advancements in AI have created a clear path to get to those capabilities. That kind of natural interaction with humans has real safety implications. \nThe Atlas product, which was also revealed onstage Monday and will eventually head to Hyundai’s factory, has 56 degrees of freedom with rotation joints and human-scale hands that have tactile sensing. And it’s strong. The Atlas robot can lift up to 110 pounds and is designed to perform repetitive movements. \nWith that kind of dexterity and strength, it will be critical for Atlas, or any humanoid robot, to safely interact and work with humans. Some of that has been handled on the hardware side; Atlas, for instance, has 360-degree cameras to allow it to see when people are approaching. But DeepMind’s work could help the robots learn how to act.\n“Rather than having a set of predefined, loaded tasks onto the robot, we think robots should understand the physical world the same way we do,”Parada said. “They should be able to learn from their experience. Should be able to generalize new situations and get better over time. So whether it is to assemble a new car part or to tie your shoelaces, robots should learn the same way we do from a handful of examples, and then get better very quickly with a little bit of practice.”\nHyundai, which plans to bring Atlas to its factory this year and eventually deploy them for tasks like parts sequencing by 2028, has also developed protocols to increase safety and efficiency.\nHyundai said Monday it is opening a U.S. facility this year called a Robot Metaplant Application Center, or RMAC, that will teach robots how to map movements like lifts and turns. Training data from RMAC will be combined with real-world data collected via a software platform used in its Georgia factory to continually improve the robots.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here. \nThis article was updated to include more information about Atlas’ specs. \n\n\n\t\tKirsten Korosec is a reporter and editor who has covered the future of transportation from EVs and autonomous vehicles to urban air mobility and in-car tech for more than a decade. She is currently the transportation editor at TechCrunch and co-host of TechCrunch’s Equity podcast. She is also co-founder and co-host of the podcast, “The Autonocast.” She previously wrote for Fortune, The Verge, Bloomberg, MIT Technology Review and CBS Interactive.\nYou can contact or verify outreach from Kirsten by emailing kirsten.korosec@techcrunch.com or via encrypted message at kkorosec.07 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Boston Dynamics’ next-gen humanoid robot will have Google DeepMind DNA",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/atlas-announcement.jpg?w=150"
      ],
      "datePublished": "2026-01-05T21:55:55.000Z",
      "dateModified": "2026-01-05T21:55:55.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Nvidia launches powerful new Rubin chip architecture",
    "url": "https://techcrunch.com/2026/01/05/nvidia-launches-powerful-new-rubin-chip-architecture/",
    "image": "https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T22:16:58.000Z",
    "description": "\nToday at the Consumer Electronics Show, Nvidia CEO Jensen Huang officially launched the company’s new Rubin computing architecture, which he describe",
    "body": "\nToday at the Consumer Electronics Show, Nvidia CEO Jensen Huang officially launched the company’s new Rubin computing architecture, which he described as the state of the art in AI hardware. The new architecture is currently in production and is expected to ramp up further in the second half of the year.\n“Vera Rubin is designed to address this fundamental challenge that we have: The amount of computation necessary for AI is skyrocketing.” Huang told the audience. “Today, I can tell you that Vera Rubin is in full production.”\nThe Rubin architecture, which was first announced in 2024, is the latest result of Nvidia’s relentless hardware development cycle, which has transformed Nvidia into the most valuable corporation in the world. The Rubin architecture will replace the Blackwell architecture, which in turn, replaced the Hopper and Lovelace architectures.\n\n\n\nRubin chips are already slated for use by nearly every major cloud provider, including high-profile Nvidia partnerships with Anthropic, OpenAI, and Amazon Web Services. Rubin systems will also be used in HPE’s Blue Lion supercomputer and the upcoming Doudna supercomputer at Lawrence Berkeley National Lab.\nNamed for the astronomer Vera Florence Cooper Rubin, the Rubin architecture consists of six separate chips designed to be used in concert. The Rubin GPU stands at the center, but the architecture also addresses growing bottlenecks in storage and interconnection with new improvements in the Bluefield and NVLink, systems respectively. The architecture also includes a new Vera CPU, designed for agentic reasoning.\nExplaining the benefits of the new storage, Nvidia’s senior director of AI infrastructure solutions Dion Harris pointed to the growing cache-related memory demands of modern AI systems. \n“As you start to enable new types of workflows, like agentic AI or long-term tasks, that puts a lot of stress and requirements on your KV cache,” Harris told reporters on a call, referring to a memory system used by AI models to condense inputs. “So we’ve introduced a new tier of storage that connects externally to the compute device, which allows you to scale your storage pool much more efficiently.”\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAs expected, the new architecture also represents a significant advance in speed and power efficiency. According to Nvidia’s tests, the Rubin architecture will operate three and a half times faster than the previous Blackwell architecture on model-training tasks and five times faster on inference tasks, reaching as high as 50 petaflops. The new platform will also support eight times more inference compute per watt.\nRubin’s new capabilities come amid intense competition to build AI infrastructure, which has seen both AI labs and cloud providers scramble for Nvidia chips as well as the facilities necessary to power them. On an earnings call in October 2025, Huang estimated that between $3 trillion and $4 trillion will be spent on AI infrastructure over the next five years.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here.\n\nWatch Nvidia CEO Jensen Huang reveal what he described as the state of the art in AI hardware: the new Rubin computing architecture.“Vera Rubin is designed to address this fundamental challenge that we have: The amount of computation necessary for AI is skyrocketing.” Huang… pic.twitter.com/MhGVqytX04— TechCrunch (@TechCrunch) January 5, 2026\n\n\n\n\t\n\t\tRussell Brandom has been covering the tech industry since 2012, with a focus on platform policy and emerging technologies. He previously worked at The Verge and Rest of World, and has written for Wired, The Awl and MIT’s Technology Review.\r\n He can be reached at russell.brandom@techcrunch.com or on Signal at 412-401-5489.\t\n\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Nvidia launches powerful new Rubin chip architecture",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?w=150"
      ],
      "datePublished": "2026-01-05T22:16:58.000Z",
      "dateModified": "2026-01-05T22:16:58.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Hacktivist deletes white supremacist websites live onstage during hacker conference",
    "url": "https://techcrunch.com/2026/01/05/hacktivist-deletes-white-supremacist-websites-live-on-stage-during-hacker-conference/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/martha-root-whitedate-hack-screenshot1-e1767638022105.png?w=150",
    "tag": "Tech",
    "date": "2026-01-05T18:57:31.000Z",
    "description": "\nA hacktivist remotely wiped three white supremacist websites live onstage during their talk at a hacker conference last week, with the sites yet to r",
    "body": "\nA hacktivist remotely wiped three white supremacist websites live onstage during their talk at a hacker conference last week, with the sites yet to return online.\nThe pseudonymous hacker, who goes by Martha Root — dressed as Pink Ranger from the Power Rangers — deleted the servers of WhiteDate, WhiteChild, and WhiteDeal in real time at the end of a talk at the annual Chaos Communication Congress in Hamburg, Germany. \nRoot gave the talk alongside journalists Eva Hoffmann and Christian Fuchs, who wrote an article about the hacked sites for the German weekly paper Die Zeit in October. \nAs of this writing, WhiteDate, which Hoffmann described as a “Tinder for Nazis”; WhiteChild, a site that claimed to match white supremacists’ sperm and egg donors; and WhiteDeal, a sort-of Taskrabbit-esque labor marketplace for racists, are all offline.\nThe administrator of the three websites confirmed the hack on their social media accounts. \n“They publicly delete all my websites while the audience rejoices. This is cyberterrorism,” the administrator wrote on X on Sunday, vowing repercussions.\nThe administrator also claimed that Root deleted their X account before it was restored.\n\n‼️A German hacker known as \"Martha Root\" dressed as a pink Power Ranger and deleted a white supremacist dating website live onstageThis happened during the recent CCC conference.Martha had infiltrated the site, ran her own AI chatbot to extract as much information from users… pic.twitter.com/vpTEoFR8JR— International Cyber Digest (@IntCyberDigest) January 2, 2026\n\nRoot also published the data allegedly scraped from WhiteDate online. \nThe hacker said that they scraped WhiteDate’s public data and found “poor cybersecurity hygiene that would make even your grandma’s AOL account blush.” Root said that users’ images included precise geolocation metadata that “practically hands out home addresses with a side of awkward selfies.” \n“Imagine calling yourselves the ‘master race’ but forgetting to secure your own website — maybe try mastering to host WordPress before world domination,” Root wrote. \nThe leaked data includes users’ profiles with name, pictures, description, age, location (both containing precise coordinates and user-set country and state), gender, language, race, and other personal information that users uploaded. Root wrote on the site that “for now” there are no emails, passwords, or private conversations. \nAccording to the leaked data, WhiteDate had more than 6,500 users, of which 86% were men and 14% women. “A gender ratio that makes the Smurf village look like a feminist utopia,” Root wrote.\nRoot infiltrated the sites using AI chatbots that bypassed verification processes and were verified as “white,” according to the talks’ abstract. \nDDoSecrets, a nonprofit collective that stores leaked datasets in the public interest, announced that it has received “files and user information” from the three white supremacist websites. The collective, which calls this release “WhiteLeaks,” has not publicly released the data but is instead asking verified journalists and researchers to request access to the full 100 gigabyte dataset.\nThe administrator of the three websites did not immediately respond to TechCrunch’s request for comment, which was sent to an email address shown during the conference talk. TechCrunch also sent an email to an address that appears on the public domain records of two of the three websites. The person behind that address also did not immediately respond to our email.\nRoot, Hoffmann, and Fuchs claim to have identified the real identity of the websites’ administrator as a woman from Germany. TechCrunch could not independently confirm the identity of the administrator.\n\n\n\t\tLorenzo Franceschi-Bicchierai is a Senior Writer at TechCrunch, where he covers hacking, cybersecurity, surveillance, and privacy. \nYou can contact or verify outreach from Lorenzo by emailing lorenzo@techcrunch.com, via encrypted message at +1 917 257 1382 on Signal, and @lorenzofb on Keybase/Telegram.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Hacktivist deletes white supremacist websites live onstage during hacker conference",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/martha-root-whitedate-hack-screenshot1-e1767638022105.png?w=150"
      ],
      "datePublished": "2026-01-05T18:57:31.000Z",
      "dateModified": "2026-01-05T18:57:31.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  }
]