[
  {
    "title": "LG’s new OLED TV is just 9mm thick",
    "url": "https://techcrunch.com/2026/01/05/lgs-new-oled-tv-is-just-9mm-thick/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/LG-OLED-evo-W6_main.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T12:13:11.000Z",
    "description": "\nA significant portion of the annual Consumer Electronics Show (CES) is about TVs, and this year, LG is showing off its manufacturing chops with a new",
    "body": "\nA significant portion of the annual Consumer Electronics Show (CES) is about TVs, and this year, LG is showing off its manufacturing chops with a new Wallpaper OLED TV that is just 9mm thick.\nThe South Korean company launched the Wallpaper line in 2017, and is now bringing it back with this model, dubbed OLED evo W6. The company says you can connect the TV to its Zero Connect Box wirelessly to stream lossless 4K video and audio, provided the box is within 10 meters of the TV.\nLG claims the TV improves on brightness, color, and blacks compared to its predecessor, and is certified “reflection-free” by product testing group Intertek. The display has a refresh rate maxing out at 165Hz, and supports AMD’s FreeSync Premium tech.\nLG didn’t provide any pricing details, but it said that the W6 will be available in 77- and 83-inch sizes.\n\n\tTopics\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "LG’s new OLED TV is just 9mm thick",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/LG-OLED-evo-W6_main.jpg?w=150"
      ],
      "datePublished": "2026-01-05T12:13:11.000Z",
      "dateModified": "2026-01-05T12:13:11.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "AMD unveils new AI PC processors for general use and gaming at CES",
    "url": "https://techcrunch.com/2026/01/05/amd-unveils-new-ai-pc-processors-for-general-use-and-gaming-at-ces/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Ryzen-AI-Blog-1200x675-1.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-06T03:30:00.000Z",
    "description": "\nAMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone. \nAs part of that promi",
    "body": "\nAMD Chair and CEO Lisa Su kicked off her keynote at CES 2026 with a message about what compute could deliver: AI for everyone. \nAs part of that promise, AMD announced a new line of AI processors as the company thinks AI-powered personal computers are the way of the future.\nThe semiconductor giant revealed AMD Ryzen AI 400 Series processor, its latest version of its AI-powered PC chips, at the yearly CES conference on Monday. The company says the latest version of its Ryzen processor series allows for 1.3x faster multitasking than its competitors and are 1.7x times faster at content creation.\nThese new chips feature 12 CPU Cores, individual processing units inside a core processor, and 24 threads, independent streams of instruction\nThis is an upgrade to the Ryzen AI 300 Series processor that was announced in 2024. AMD started producing the Ryzen processor series in 2017.\nRahul Tikoo, senior vice president and general manager of AMD’s client business, said AMD has expanded to over 250 AI PC platforms on the company’s recent press briefing. That represents a growth 2x over the last year, he added.\n“In the years ahead, AI is going to be a multi-layered fabric that gets woven into every level of computing at the personal layer,” Tikoo said. “Our AI PCs and devices will transform how we work, how we play, how we create and how we connect with each other.”\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAMD also announced the release of the AMD Ryzen 7 9850X3D, the latest version of its gaming-focused processor.\n“No matter who you are and how you use technology on a daily basis, AI is reshaping everyday computing,” Tikoo said. “You have thousands of interactions with your PC every day. AI is able to understand, learn context, bring automation, provide deep reasoning and personal customization to every individual.”\nPCs that include either the Ryzen AI 300 Series processor or the AMD Ryzen 7 9850X3D processor become available in the first quarter of 2026.\nThe company also announced the latest version of its Redstone ray tracing technology, which simulates physical behavior of light, which allows for better video game graphics without a performance or speed lag.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here. \n\n\n\t\tBecca is a senior writer at TechCrunch that covers venture capital trends and startups. She previously covered the same beat for Forbes and the Venture Capital Journal.\r\nYou can contact or verify outreach from Becca by emailing rebecca.szkutak@techcrunch.com.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "AMD unveils new AI PC processors for general use and gaming at CES",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/Ryzen-AI-Blog-1200x675-1.jpg?w=150"
      ],
      "datePublished": "2026-01-06T03:30:00.000Z",
      "dateModified": "2026-01-06T03:30:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Lego Smart Bricks introduce a new way to build — and they don’t require screens",
    "url": "https://techcrunch.com/2026/01/05/lego-smart-bricks-introduce-a-new-way-to-build-and-they-dont-require-screens/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/SMARTBrick_16x9.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T20:31:51.000Z",
    "description": "\n\t\n\t\tImage Credits:Lego\t\n\t\n\t\t\t\t\t\t12:31 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLego announced its new Smart Play system at CES 2026 on Monday, adding",
    "body": "\n\t\n\t\tImage Credits:Lego\t\n\t\n\t\t\t\t\t\t12:31 PM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLego announced its new Smart Play system at CES 2026 on Monday, adding interactive, responsive Legos to the famously analog franchise.\nThe Smart Play system includes a 2×4 brick, Smart Tag tiles, and Smart Minifigures. The Smart Bricks and Minifigures can sense nearby Smart Tags, which are 2×2 studless tiles with unique digital IDs that tell the Bricks and Minifigures how to act.\nIf the Smart Tag comes in a set for building a helicopter, for example, then the Smart Brick will light up and make propeller sounds that would help bring a helicopter to life. Its built-in accelerometer would make these lights and sounds more consistent with how you’re actually playing with the helicopter, since the Brick will be able to sense when the helicopter is zooming through the sky or turned upside down.\nImage Credits:LEGO\nThe Smart Bricks are powered by a patented ASIC chip, which is smaller than the size of a single Lego stud. The chip uses near-field magnetic positioning to recognize the Tags around it, as well as a miniature speaker, accelerometer, and LED array. Lego also developed a Bluetooth-based protocol called BrickNet, which allows multiple Smart Bricks to recognize each other and operate in tandem. The company claims that BrickNet is protected by enhanced encryption and privacy controls (all of which is necessary, but imagine a world where hacking into toys wasn’t a concern!).\nThere’s no setup required to pair the elements of the Smart Play system, making it easy for kids to get started — and parents will be pleased to note that there are no screens involved in the Smart system at all. However, Lego’s website says that there will be a Smart Tag for animating Lego toilets, so… there’s that.\nLego’s first two Smart Play sets — which are both Star Wars-themed — will launch on March 1, though preorders open on Friday. The “Luke’s Red Five X-wing” building set will retail for $69.99, while the larger “Throne Room Duel and A-wing” set will cost $159.99. These sets use the Smart Play system to animate characters like Luke Skywalker and Princess Leia, allowing them to interact with Smart Tags, which enable Lightsaber duels among other Star Wars-related capabilities.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\n\n\t\t\t\n\tTopics\n\n\n\n\t\tAmanda Silberling is a senior writer at TechCrunch covering the intersection of technology and culture. She has also written for publications like Polygon, MTV, the Kenyon Review, NPR, and Business Insider. She is the co-host of Wow If True, a podcast about internet culture, with science fiction author Isabel J. Kim. Prior to joining TechCrunch, she worked as a grassroots organizer, museum educator, and film festival coordinator. She holds a B.A. in English from the University of Pennsylvania and served as a Princeton in Asia Fellow in Laos.\nYou can contact or verify outreach from Amanda by emailing amanda@techcrunch.com or via encrypted message at @amanda.100 on Signal. \t\n\t\n\t\tView Bio \n\t\n\n\t\t",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Lego Smart Bricks introduce a new way to build — and they don’t require screens",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/SMARTBrick_16x9.jpg?w=150"
      ],
      "datePublished": "2026-01-05T20:31:51.000Z",
      "dateModified": "2026-01-05T20:31:51.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Lucid Motors doubled EV output in 2025 after early Gravity SUV struggles",
    "url": "https://techcrunch.com/2026/01/05/lucid-motors-doubled-ev-output-in-2025-after-early-gravity-suv-struggles/",
    "image": "https://techcrunch.com/wp-content/uploads/2025/04/lucid-gravity-main.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T16:51:18.000Z",
    "description": "\n\t\n\t\tImage Credits:Lucid Group\t\n\t\n\t\t\t\t\t\t8:51 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLucid Motors built twice as many electric vehicles in 2025 as it",
    "body": "\n\t\n\t\tImage Credits:Lucid Group\t\n\t\n\t\t\t\t\t\t8:51 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nLucid Motors built twice as many electric vehicles in 2025 as it did in the prior year, a sign that the company has bounced back from early production struggles with its new Gravity SUV. \nThe company announced Monday that it finished the year having built 18,378 EVs, with 8,412 of those coming in the fourth quarter alone. That’s more than Lucid built at its Casa Grande, Arizona factory in the first half of the year. Lucid also said it delivered — meaning sold — 15,841 vehicles across the whole year, a 55% increase over 2024’s figures.\nThe stronger finish to 2025 sets Lucid up for an all-important year that will see the company start building the first vehicle on its new mid-sized EV platform. The company has said this first vehicle will cost around $50,000, putting it near the same part of the market as the Tesla Model Y and Rivian’s upcoming R2 SUV.\nThe numbers still pale in comparison to the projections Lucid Motors threw around when it went public in a $4 billion reverse merger in 2021. At that time, the company claimed it would deliver 135,000 vehicles in 2025, with 86,000 of those being Gravity SUVs, 42,000 being Air sedans, and the remaining 7,000 coming from its yet-to-debut mid-sized EV. \nThose targets quickly became unrealistic as Lucid ran into production, supply, and demand challenges for both of its vehicles, all while navigating an automotive market that was severely disrupted by the pandemic. The company particularly struggled in early 2025 as it started to ramp up production of the Gravity SUV. It has since been dealing with a number of quality issues on the vehicle, to the level that interim CEO Marc Winterhoff sent an email to customers in December saying that he shared in their “frustration.”\n“Lingering software problems have unfortunately affected our customers’ experience and satisfaction. I would like to assure you that we are laser focused on addressing these issues,” he wrote.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\n\n\t\t\t\n\tTopics\n\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n\n\t\t",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Lucid Motors doubled EV output in 2025 after early Gravity SUV struggles",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2025/04/lucid-gravity-main.jpg?w=150"
      ],
      "datePublished": "2026-01-05T16:51:18.000Z",
      "dateModified": "2026-01-05T16:51:18.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Insight Partners sued by former vice president Kate Lowry",
    "url": "https://techcrunch.com/2026/01/05/insight-partners-sued-by-former-vice-president-kate-lowry/",
    "image": "https://techcrunch.com/wp-content/uploads/2023/11/gavel-messy-legal.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:30:00.000Z",
    "description": "\nKate Lowry, a former vice president at Insight Partners, is suing the firm, alleging disability discrimination, gender discrimination, and wrongful t",
    "body": "\nKate Lowry, a former vice president at Insight Partners, is suing the firm, alleging disability discrimination, gender discrimination, and wrongful termination, according to a suit filed on December 30 in San Mateo County, California, and seen by TechCrunch. Insight Partners did not immediately respond to TechCrunch’s request for comment.\nLowry told TechCrunch she filed the suit because she believes “too many powerful, wealthy people in venture act like it’s OK to break the law and systemically underpay and abuse their employees.”\n“It’s an oppressive system that reflect[s] broader trends in society that use fear, intimidation, and power to silence and isolate truth. I’m trying to change that.”\nLowry began working at Insight Partners in 2022, after previously working for Meta, McKinsey & Company, and an early-stage startup. The suit alleges that, upon being hired, she was assigned to a different supervisor than the person mentioned during her interview.  \nShe alleges in the suit that she was told by her new supervisor, who was a woman, to be “online all the time, including PTO, holidays, and weekends,” and to respond between “6 a.m. and 11 p.m. daily.”  \nLowry says in the suit that this first supervisor “berated, hazed, and antagonized” her, spoke openly about a hazing that would be “longer and more intense” than what she put other male reports through.  \nSome comments the supervisor allegedly made, according to the suit, include “you are incompetent, shut up and take notes” and “you need to obey me like a dog; do whatever I say whenever I say it, without speaking.”  Lowry also alleges that her supervisor assigned her “redundant tasks” and restricted her ability to participate in calls, while allowing less experienced male colleagues to do so. Lowry, instead, she alleges, was relegated to “administrative tasks such as note-taking and cataloging.”  \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nLowry said she became “increasingly ill” because of the work environment and that her physician advised a medical leave of absence, which she was granted and took from February to July 2023.  \nWhen she returned to work, she was placed on a new team and, the suit alleges, was told by the head of human resources that “if the new team did not like her, she would be fired.”  \nIn September 2023, Lowry said she got a concussion and took another medical leave and returned to work near the end of 2024. Due to some departures, she was placed under the supervision of a new person, where Lowry said her poor treatment continued. She also alleges that in 2024, her compensation was about 30% below the market. \nBy April 2025, she alleges she was told her compensation would be cut. In May of 2025, through her attorneys, Lowry sent a letter to Insight regarding her alleged treatment by the company. A week later, the firm terminated her employment, the suit states.  \nThe lawsuit is reminiscent of Ellen Pao’s suit against Kleiner Perkins back in 2012, in which she alleged discrimination and retaliation. That suit offered what was, at the time, a rare glimpse into how women partners felt they were treated in venture capital. Though Pao lost that suit, it sent waves through the industry, and other women went on to sue major tech companies.  \n\n\n\t\tDominic-Madori Davis is a senior venture capital and startup reporter at TechCrunch. She is based in New York City.\r\nYou can contact or verify outreach from Dominic by emailing dominic.davis@techcrunch.com or via encrypted message at +1 646 831-7565 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Insight Partners sued by former vice president Kate Lowry",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2023/11/gavel-messy-legal.jpg?w=150"
      ],
      "datePublished": "2026-01-05T23:30:00.000Z",
      "dateModified": "2026-01-05T23:30:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Nvidia wants to be the Android of generalist robotics ",
    "url": "https://techcrunch.com/2026/01/05/nvidia-wants-to-be-the-android-of-generalist-robotics/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-05-at-5.03.42-PM.png?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:00:00.000Z",
    "description": "\nNvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to ",
    "body": "\nNvidia released a new stack of robot foundation models, simulation tools, and edge hardware at CES 2026, moves that signal the company’s ambition to become the default platform for generalist robotics, much as Android became the operating system for smartphones. \nNvidia’s move into robotics reflects a broader industry shift as AI moves off the cloud and into machines that can learn how to think in the physical world, enabled by cheaper sensors, advanced simulation, and AI models that increasingly can generalize across tasks. \nNvidia revealed details on Monday about its full-stack ecosystem for physical AI, including new open foundation models that allow robots to reason, plan, and adapt across many tasks and diverse environments, moving beyond narrow task-specific bots, all of which are available on Hugging Face. \nThose models include: Cosmos Transfer 2.5 and Cosmos Predict 2.5, two world models for synthetic data generation and robot policy evaluation in simulation; Cosmos Reason 2, a reasoning vision language model (VLM) that allows AI systems to see, understand, and act in the physical world; and Isaac GR00T N1.6, its next-gen vision language action (VLA) model purpose-built for humanoid robots. GR00T relies on Cosmos Reason as its brain, and it unlocks whole-body control for humanoids so they can move and handle objects simultaneously. \nNvidia also introduced Isaac Lab-Arena at CES, an open source simulation framework hosted on GitHub that serves as another component of the company’s physical AI platform, enabling safe virtual testing of robotic capabilities.\nThe platform promises to address a critical industry challenge: As robots learn increasingly complex tasks, from precise object handling to cable installation, validating these abilities in physical environments can be costly, slow, and risky. Isaac Lab-Arena tackles this by consolidating resources, task scenarios, training tools, and established benchmarks like Libero, RoboCasa, and RoboTwin, creating a unified standard where the industry previously lacked one.\nSupporting the ecosystem is Nvidia OSMO, an open source command center that serves as connective infrastructure that integrates the entire workflow from data generation through training across both desktop and cloud environments. \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAnd to help power it all, there’s the new Blackwell-powered Jetson T4000 graphics card, the newest member of the Thor family. Nvidia is pitching it as a cost-effective on-device compute upgrade that delivers 1200 teraflops of AI compute and 64 gigabytes of memory while running efficiently at 40 to 70 watts. \nNvidia is also deepening its partnership with Hugging Face to let more people experiment with robot training without needing expensive hardware or specialized knowledge. The collaboration integrates Nvidia’s Isaac and GR00T technologies into Hugging Face’s LeRobot framework, connecting Nvidia’s 2 million robotics developers with Hugging Face’s 13 million AI builders. The developer platform’s open source Reachy 2 humanoid now works directly with Nvidia’s Jetson Thor chip, letting developers experiment with different AI models without being locked into proprietary systems.  \nThe bigger picture here is that Nvidia is trying to make robotics development more accessible, and it wants to be the underlying hardware and software vendor powering it, much like Android is the default for smartphone makers.\nThere are early signs that Nvidia’s strategy is working. Robotics is the fastest growing category on Hugging Face, with Nvidia’s models leading downloads. Meanwhile robotics companies, from Boston Dynamics and Caterpillar to Franka Robots and NEURA Robotics, are already using Nvidia’s tech.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here.\n\n\n\n\t\tRebecca Bellan is a senior reporter at TechCrunch where she covers the business, policy, and emerging trends shaping artificial intelligence. Her work has also appeared in Forbes, Bloomberg, The Atlantic, The Daily Beast, and other publications. \r\nYou can contact or verify outreach from Rebecca by emailing rebecca.bellan@techcrunch.com or via encrypted message at rebeccabellan.491 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Nvidia wants to be the Android of generalist robotics ",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-05-at-5.03.42-PM.png?w=150"
      ],
      "datePublished": "2026-01-05T23:00:00.000Z",
      "dateModified": "2026-01-05T23:00:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "This is Uber’s new robotaxi from Lucid and Nuro",
    "url": "https://techcrunch.com/2026/01/05/this-is-ubers-new-robotaxi-from-lucid-and-nuro/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/uber-lucid-gravity.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:00:04.000Z",
    "description": "\nUber, Lucid Motors, and Nuro have revealed the production-intent version of their collaborative robotaxi at the 2026 Consumer Electronics Show, and T",
    "body": "\nUber, Lucid Motors, and Nuro have revealed the production-intent version of their collaborative robotaxi at the 2026 Consumer Electronics Show, and TechCrunch got a sneak peek ahead of the reveal.\nIt’s a vehicle that’s been in the works for more than half a year now, part of a deal that saw Uber invest $300 million into Lucid and commit to buying 20,000 of the company’s EVs. On Monday, the companies said the robotaxi is already being tested on public roads ahead of a planned commercial service launching in the San Francisco Bay Area later this year.\nBased on the Lucid Gravity SUV, the robotaxi has high-resolution cameras, solid state lidar sensors, and radars integrated into the body and the roof-mounted “halo.” The autonomy package is powered by Nvidia’s Drive AGX Thor computer. That halo also has integrated LED lights that will help riders identify their vehicle (similar to how Waymo’s Jaguar I-Pace SUVs work).\nCrucially, all of this extra tech is added to the Gravity as it’s being built at Lucid Motors’ Casa Grande, Arizona factory, saving the companies some time and money. By comparison, Waymo currently has to take apart the I-Pace SUVs it receives from Jaguar and integrate the autonomous tech as it puts them back together. (Future Waymo vehicles are planned to be more purpose-built.)\nImage Credits:Sean O'Kane\nThe vehicle unveiled on Monday is a more polished-up version of the test version that the three companies have spent the last seven months showing off in press photos. The newest element revealed at CES has to do with how users will interface with the Uber-Lucid-Nuro robotaxi. That includes a small screen on the halo meant to greet riders and a ride interface inside the cabin.\nAnyone who has ridden in a Waymo will find this UI experience familiar. The rear passenger screen shows an isometric graphical view of the robotaxi moving through city streets, with representations of nearby cars and pedestrians. \nThe companies did not have an interactive version of the software — which is being created by Uber — ready to test out just yet. But it has been built to show the standard information like estimated drop-off time, how much ride time is remaining, and climate and music controls. There are also buttons to reach rider support and to tell the robotaxi to pull over.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nThe front passenger screen shows a lot of the same information, just on a larger central touchscreen display. In the demonstration car on display at the Fontainebleau hotel, a lot of the same elements appeared on the Gravity’s sweeping 34-inch curved OLED display, which sits behind the steering wheel. \nUber chose to build this forthcoming “premium” robotaxi service around the Gravity, and at a high level it seems like a wise decision. The Gravity is immensely spacious inside, especially in the two-row configuration on display at the hotel. (Uber says a three-row version will be available, too.) \nThat said, the Gravity’s first full year came with struggles. Lucid fought with software issues as it ramped up production of the SUV, and the problems got bad enough that interim CEO Marc Winterhoff sent an email to owners in December apologizing for the “frustrations” they experienced. \nLucid has seemingly been able to bounce back from that, and on Monday announced that it doubled its 2024 production figures and reached new sales records. Time will tell if the robotaxi version has any of the same kinds of software struggles.\nUber, Lucid, and Nuro said Monday that once final validation is complete on the robotaxi later this year, true production versions will start rolling off Lucid’s factory lines in Arizona. The companies did not give a concrete timeline for that, though.\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "This is Uber’s new robotaxi from Lucid and Nuro",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/uber-lucid-gravity.jpg?w=150"
      ],
      "datePublished": "2026-01-05T23:00:04.000Z",
      "dateModified": "2026-01-05T23:00:04.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Nvidia launches Alpamayo, open AI models that allow autonomous vehicles to ‘think like a human’",
    "url": "https://techcrunch.com/2026/01/05/nvidia-launches-alpamayo-open-ai-models-that-allow-autonomous-vehicles-to-think-like-a-human/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/Alpamayo-Image.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T21:52:22.000Z",
    "description": "\nAt CES 2026, Nvidia launched Alpamayo, a new family of open source AI models, simulation tools, and datasets for training physical robots and vehicle",
    "body": "\nAt CES 2026, Nvidia launched Alpamayo, a new family of open source AI models, simulation tools, and datasets for training physical robots and vehicles that are designed to help autonomous vehicles reason through complex driving situations.\n“The ChatGPT moment for physical AI is here – when machines begin to understand, reason, and act in the real world,” Nvidia CEO Jensen Huang said in a statement. “Alpamayo brings reasoning to autonomous vehicles, allowing them to think through rare scenarios, drive safely in complex environments, and explain their driving decisions.” \nAt the core of Nvidia’s new family is Alpamayo 1, a 10 billion-parameter chain-of-thought, reason-based vision language action (VLA) model that allows an AV to think more like a human so it can solve complex edge cases — like how to navigate a traffic light outage at a busy intersection — without previous experience. \n“It does this by breaking down problems into steps, reasoning through every possibility, and then selecting the safest path,” Ali Kani, Nvidia’s vice president of automotive, said Monday during a press briefing. \nOr as Huang put it during his keynote on Monday: “Not only does [Alpamayo] take sensor input and activate steering wheel, brakes, and acceleration, it also reasons about what action it’s about to take. It tells you what action it’s going to take, the reasons by which it came about that action. And then, of course, the trajectory.”\nAlpamayo 1’s underlying code is available on Hugging Face. Developers can fine-tune Alpamayo into smaller, faster versions for vehicle development, use it to train simpler driving systems, or build tools on top of it like auto-labeling systems that automatically tag video data or evaluators that check if a car made a smart decision. \n“They can also use Cosmos to generate synthetic data and then train and test their Alpamayo-based AV application on the combination of the real and synthetic dataset,” Kani said. Cosmos is Nvidia’s brand of generative world models, AI systems that create a representation of a physical environment so they can make predictions and take actions. \n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nAs part of the Alpamayo rollout, Nvidia is also releasing an open dataset with more than 1,700 hours of driving data collected across a range of geographies and conditions, covering rare and complex real-world scenarios. The company is additionally launching AlpaSim, an open source simulation framework for validating autonomous driving systems. Available on GitHub, AlpaSim is designed to recreate real-world driving conditions, from sensors to traffic, so developers can safely test systems at scale.\n\n\n\n\t\tRebecca Bellan is a senior reporter at TechCrunch where she covers the business, policy, and emerging trends shaping artificial intelligence. Her work has also appeared in Forbes, Bloomberg, The Atlantic, The Daily Beast, and other publications. \r\nYou can contact or verify outreach from Rebecca by emailing rebecca.bellan@techcrunch.com or via encrypted message at rebeccabellan.491 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Nvidia launches Alpamayo, open AI models that allow autonomous vehicles to ‘think like a human’",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/Alpamayo-Image.jpg?w=150"
      ],
      "datePublished": "2026-01-05T21:52:22.000Z",
      "dateModified": "2026-01-05T21:52:22.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Boston Dynamics’ next-gen humanoid robot will have Google DeepMind DNA",
    "url": "https://techcrunch.com/2026/01/05/boston-dynamicss-next-gen-humanoid-robot-will-have-google-deepmind-dna/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/atlas-announcement.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T21:55:55.000Z",
    "description": "\nRobotics company Boston Dynamics has struck a partnership with Google’s AI research lab to speed up the development of its next-generation humanoid r",
    "body": "\nRobotics company Boston Dynamics has struck a partnership with Google’s AI research lab to speed up the development of its next-generation humanoid robot Atlas — and make it act more human around people.\nThe partnership, which was announced Monday during the Hyundai press conference at CES 2026, is centered on robotics research that will use Google DeepMind’s AI foundation models. Boston Dynamics’ humanoid robot Atlas will be the first test case, according to Carolina Parada, senior director of robotics at Google DeepMind.\n“We’re looking to integrate our cutting-edge AI foundation models with Boston Dynamics’ new Atlas robots, and we’ll aim to develop the world’s most advanced robot foundation model to fulfill the promise of true general-purpose human needs,” Parada said onstage.\nThe tie-up comes less than a year after the Google AI research lab announced new AI models called Gemini Robotics that are designed to allow robots to perceive, reason, use tools, and interact with humans. Gemini Robotics is based on a large-scale multimodal generative AI model, Gemini. At the time, Google DeepMind said the robotics AI model was trained to generalize behavior across a range of different robotics hardware.\nEnter Boston Dynamics, and its majority owner, Hyundai Motor Group. While accelerating research will be a central piece of this partnership, this has real-world scaling intent.\nBoston Dynamics already has products, like the quadruped Spot, that are in customers’ hands in more than 40 countries. Its warehouse robot Stretch has unloaded more than 20 million boxes globally since its launch in 2023, according to Hyundai. Now Boston Dynamics and Hyundai are preparing for the next generation, starting with the humanoid robot Atlas, which the company announced Monday is already in production and headed to the Hyundai factory in Savannah, Georgia\nA prototype of Atlas walked onstage during the press conference, showing off its ability to move. But as Alberto Rodriguez, director of Atlas behavior at Boston Dynamics, noted, making “Atlas into a product requires more than athletic performance for humanoids to really deliver on their promise. They have to be able to interact with people naturally.”\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nRodriguez and his counterparts at Boston Dynamics believe that recent advancements in AI have created a clear path to get to those capabilities. That kind of natural interaction with humans has real safety implications. \nThe Atlas product, which was also revealed onstage Monday and will eventually head to Hyundai’s factory, has 56 degrees of freedom with rotation joints and human-scale hands that have tactile sensing. And it’s strong. The Atlas robot can lift up to 110 pounds and is designed to perform repetitive movements. \nWith that kind of dexterity and strength, it will be critical for Atlas, or any humanoid robot, to safely interact and work with humans. Some of that has been handled on the hardware side; Atlas, for instance, has 360-degree cameras to allow it to see when people are approaching. But DeepMind’s work could help the robots learn how to act.\n“Rather than having a set of predefined, loaded tasks onto the robot, we think robots should understand the physical world the same way we do,”Parada said. “They should be able to learn from their experience. Should be able to generalize new situations and get better over time. So whether it is to assemble a new car part or to tie your shoelaces, robots should learn the same way we do from a handful of examples, and then get better very quickly with a little bit of practice.”\nHyundai, which plans to bring Atlas to its factory this year and eventually deploy them for tasks like parts sequencing by 2028, has also developed protocols to increase safety and efficiency.\nHyundai said Monday it is opening a U.S. facility this year called a Robot Metaplant Application Center, or RMAC, that will teach robots how to map movements like lifts and turns. Training data from RMAC will be combined with real-world data collected via a software platform used in its Georgia factory to continually improve the robots.\nFollow along with all of TechCrunch’s coverage of the annual CES conference here. \nThis article was updated to include more information about Atlas’ specs. \n\n\n\t\tKirsten Korosec is a reporter and editor who has covered the future of transportation from EVs and autonomous vehicles to urban air mobility and in-car tech for more than a decade. She is currently the transportation editor at TechCrunch and co-host of TechCrunch’s Equity podcast. She is also co-founder and co-host of the podcast, “The Autonocast.” She previously wrote for Fortune, The Verge, Bloomberg, MIT Technology Review and CBS Interactive.\nYou can contact or verify outreach from Kirsten by emailing kirsten.korosec@techcrunch.com or via encrypted message at kkorosec.07 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Boston Dynamics’ next-gen humanoid robot will have Google DeepMind DNA",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/atlas-announcement.jpg?w=150"
      ],
      "datePublished": "2026-01-05T21:55:55.000Z",
      "dateModified": "2026-01-05T21:55:55.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "The 2026 BMW iX3 voice assistant will be powered by Alexa+",
    "url": "https://techcrunch.com/2026/01/05/the-2026-bmw-ix3-voice-assistant-will-be-powered-by-alexa/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/bmw-amazon.jpeg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T17:00:00.000Z",
    "description": "\n\t\n\t\tImage Credits:Amazon\t\n\t\n\t\t\t\t\t\t9:00 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nBMW is finally getting the next-generation Alexa voice assistant and ",
    "body": "\n\t\n\t\tImage Credits:Amazon\t\n\t\n\t\t\t\t\t\t9:00 AM PST · January 5, 2026\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\nBMW is finally getting the next-generation Alexa voice assistant and it’s coming with a generative AI upgrade. \nAmazon said Monday that the 2026 BMW iX3 will be equipped with Alexa+, the same generative AI tech that launched in millions of the e-commerce giant’s smart devices last year. This will be the first vehicle to come with Amazon’s next-generation voice assistant, the companies announced during the 2026 Consumer Electronics Show in Las Vegas.\nThe launch is one part of Amazon’s effort to bring its LLM-powered voice and digital assistant to every device — whether handheld or in the driver’s seat — touched by consumers. Alexa+ is already in more than 600 million devices. And automotive is next on the list.\nBringing a custom version of Alexa+ into the BMW iX3 will be an important test for Amazon. Automakers have struggled for years to bring a voice assistant inside vehicles that can handle complex functions and requests that don’t end with the driver yelling in frustration. Efforts to develop natural language processing — a form of AI that lets computers understand and respond to human speech — have been in the works for more than a decade. And while progress has been made, these systems can still be easily stumped by humans.\nBMW and Amazon’s Alexa+ partnership has been three years in the making.\nBMW announced in 2022 that Amazon Alexa would be the foundation of its next-generation voice assistant. This meant BMW wouldn’t just embed Alexa into its vehicles but would also use Amazon’s technology platform known as Alexa Custom Assistant to build its own custom version. That timeline was extended as Amazon worked on an automotive version of Alexa+, an overhauled voice assistant developed and powered by large language models that promises to deliver seamless and natural conversations, like talking to a human.\nAlexa+ was built using Amazon Bedrock, a service that lets AWS customers build apps using generative AI models from Amazon and other third-party partners. Customers, like BMW, can then customize the app with their proprietary data.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nThe end result, according to Amazon, is a voice assistant that can break down complex requests, reason through steps, and take actions across different services. For instance, Amazon says users can start a conversation with their Alexa+-enabled Echo speaker in the home and continue it in their BMW. Once in the car, the user can make requests through the Alexa+ assistant that might normally require opening up different apps, like music, navigation, and a home security system.\n\n\t\t\t\n\tTopics\n\n\n\n\t\tKirsten Korosec is a reporter and editor who has covered the future of transportation from EVs and autonomous vehicles to urban air mobility and in-car tech for more than a decade. She is currently the transportation editor at TechCrunch and co-host of TechCrunch’s Equity podcast. She is also co-founder and co-host of the podcast, “The Autonocast.” She previously wrote for Fortune, The Verge, Bloomberg, MIT Technology Review and CBS Interactive.\nYou can contact or verify outreach from Kirsten by emailing kirsten.korosec@techcrunch.com or via encrypted message at kkorosec.07 on Signal.\t\n\t\n\t\tView Bio \n\t\n\n\t\t",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "The 2026 BMW iX3 voice assistant will be powered by Alexa+",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/bmw-amazon.jpeg?w=150"
      ],
      "datePublished": "2026-01-05T17:00:00.000Z",
      "dateModified": "2026-01-05T17:00:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Google previews new Gemini features for TV at CES 2026",
    "url": "https://techcrunch.com/2026/01/05/google-previews-new-gemini-features-for-tv-at-ces-2026/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/google-tv.webp?w=150",
    "tag": "Tech",
    "date": "2026-01-05T14:00:00.000Z",
    "description": "\nGoogle believes AI can improve the TV-watching experience, which is why it brought its Gemini AI to Google TV devices in November. At CES 2026 in Las",
    "body": "\nGoogle believes AI can improve the TV-watching experience, which is why it brought its Gemini AI to Google TV devices in November. At CES 2026 in Las Vegas, the company is now showing off a series of new Gemini features that will soon arrive on the TV, making it possible for viewers to deep-dive into topics, search for and “reimagine” their personal photos and videos with AI, and, perhaps best of all, tell the TV what to do instead of having to navigate through complicated settings.\nThe company is first bringing these Gemini features and others to select TCL televisions before rolling them out more broadly to other Google TV devices in the months ahead. \nDesigned for large-screen experiences, Gemini for Google TV will allow users to talk to their TV to find something to watch, catch up on a favorite series by getting a recap of the plot, or get recommendations, all by using natural language conversation. For instance, you could ask Gemini for something to watch that would be a blend of two people’s tastes, or get help remembering a show or movie where you can’t remember the title, but can describe the plot or name one of the actors. \nYou could even ask Google something like, “What’s the new hospital drama everyone’s talking about?”\nImage Credits:Google\nGemini can respond to users’ questions through a new visually rich framework that adapts to individual queries, combining text, imagery, video context, and real-time sports updates, as required. \nBut Google sees the potential for the TV’s screen to be used for more than just entertainment; with Gemini, the TV can be used to educate, too. \nAt CES, Google showed how this would work. When users ask a question about something they want to learn about, the TV screen can offer a deep dive into the topic. A narrated interactive overview simplifies concepts, and users can ask follow-up questions to learn more.\nImage Credits:Google\nGemini will also allow users to query their Google Photos library for specific people or moments. They can also apply artistic styles to their photos and videos using Gemini AI and turn their memories into cinematic slideshows, says Google.\nImage Credits:Google\nImage Credits:Google\nHowever, perhaps the most useful feature is one that gives you the power to optimize the TV’s settings using only your voice.\nNow, you’ll be able to tell Gemini things like “the screen is too dim” or “I can’t hear the dialogue,” and Gemini will adjust the relevant settings without you having to leave the movie or TV show you’re watching to dig through menus to find the necessary option. \nImage Credits:Google\nGoogle says the new Gemini features will require the Google TV devices to be running Android TV OS 14 or higher, and they will need an internet connection. Not all languages, countries, or devices will be supported at launch, and users must also have a Google account to access the Gemini for TV experience. \n\n\n\t\tSarah has worked as a reporter for TechCrunch since August 2011. She joined the company after having previously spent over three years at ReadWriteWeb. Prior to her work as a reporter, Sarah worked in I.T. across a number of industries, including banking, retail and software.\nYou can contact or verify outreach from Sarah by emailing sarahp@techcrunch.com or via encrypted message at sarahperez.01 on Signal. \t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Google previews new Gemini features for TV at CES 2026",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/google-tv.webp?w=150"
      ],
      "datePublished": "2026-01-05T14:00:00.000Z",
      "dateModified": "2026-01-05T14:00:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Luminar claims founder Austin Russell is dodging a subpoena in the bankruptcy case",
    "url": "https://techcrunch.com/2026/01/05/luminar-claims-founder-austin-russell-is-dodging-a-subpoena-in-the-bankruptcy-case/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-1741630514.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T12:10:00.000Z",
    "description": "\nLidar-maker Luminar says its founder and former CEO Austin Russell has been evading requests for information — including a subpoena — that the compan",
    "body": "\nLidar-maker Luminar says its founder and former CEO Austin Russell has been evading requests for information — including a subpoena — that the company needs in order to decide whether it should take legal action against him. \nThe company, which entered the Chapter 11 bankruptcy process in late December, said in an emergency filing over the weekend that it has been trying to reclaim company-owned devices from Russell since his resignation in May. While it has recovered six computers, Luminar is still seeking Russell’s company-issued phone and a digital copy of his personal phone.\nLuminar’s lawyers also wrote in the filing that Russell and his own personal employees repeatedly misled legal representatives about the founder’s location over the holidays. They’re asking the court for permission to instead serve Russell by mail or email. A lawyer for Luminar declined to comment further.\nIn emails attached to the filing, Russell claimed he was being cooperative and has been trying to get assurances from Luminar that any personal data from his devices will be protected. \n“The company declined, so we will follow the court-established process for data handling protections instead,” Leonard Shulman, an attorney for Russell, told TechCrunch in a statement.\nThe emergency filing is one of the first major twists in a fast-moving bankruptcy case that involves Luminar trying to sell the two main parts of its business. The company is seeking court approval of an already reached agreement to sell its semiconductor subsidiary to a company called Quantum Computing, Inc., and has established a January 9 deadline for bids on its lidar division.\nRussell, through his new venture Russell AI Labs, tried to buy Luminar before the Chapter 11 filing and has expressed plans to make a bid in the bankruptcy process. “As it relates to Luminar, our focus remains on what matters: Russell AI Labs’ bid to rebuild the company and bring value to its stakeholders,” Shulman said.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nLawyers for Luminar said in the filing that it began hunting for information from Russell in May, just after he abruptly resigned following a “code of business conduct and ethics inquiry” performed by the board’s audit committee. The company was evaluating whether it may have potential legal claims against him “relating to the Audit Committee inquiry and personal loans taken by Mr. Russell,” according to the filing. But Luminar said those efforts were unsuccessful and Russell was not cooperative.\nOn November 12, Luminar’s board of directors established a Special Investigation Committee and hired law firm Weil, Gotshal & Manges to further investigate “certain acts, omissions, transactions and potential claims and causes of action involving or related to certain current and former directors and officers of Luminar.” \nOne month later, just before the bankruptcy, lawyers for Weil contacted the law firm of McDermott Will & Schulte, which had previously represented Russell. The Weil lawyers asked about collecting Russell’s Luminar-provided laptop and desktop computers, along with his company-issued phone and a digital copy (or “image”) of his personal phone. \nThe lawyers from Weil spent a week trying to confirm whether McDermott would represent Russell in the matter of the Special Investigation Committee, only to find out on December 19 that it would not. The Weil lawyers tried contacting Russell directly instead. \nRussell responded for the first time on Christmas Eve, according to the filing. He eventually authorized McDermott to turn over the computers (which the firm had kept in its possession since his resignation), but the founder repeatedly asked for guarantees that Luminar’s lawyers would not search through personal data on his phones, emails attached to the emergency filing show. \n“I have offered direct cooperation as well as prompt action, even through the holidays – but if this singular basic protection cannot be confirmed, I am advised further deliberations on this matter will not be productive,” Russell wrote in an email on New Year’s Eve.\nRepresentatives for Luminar arranged to have a forensic examiner appear at Russell’s mansion in Florida on New Year’s Day. But the technician was turned away by Russell’s security team, which a lawyer for Luminar called “unacceptable.” \nRussell claimed the technician was sent to his home “unannounced” on the holiday morning “when I was asleep,” and reiterated his desire to protect the privacy of his personal data. A lawyer for Luminar responded that they had “repeatedly confirmed that we have no intention of looking at any documents beyond those that are Luminar-related.” Russell replied on January 2 that “[a]ny characterization that I have been uncooperative is wholly inaccurate” and accused the lawyers of “word gymnastics.” \nLuminar’s lawyers tried instead to subpoena this information from Russell but claim their process servers were similarly turned away by his security team. They also claim those security team members lied about Russell’s presence at his Florida residence. \n“Can we try to serve Austin again today? We’re going to need someone dogged. He is going to evade service as long as possible,” one of the Weil lawyers wrote in an email on New Year’s Eve. “In fact, he was home when your person tried last time and the guard simply lied for him.”\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Luminar claims founder Austin Russell is dodging a subpoena in the bankruptcy case",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-1741630514.jpg?w=150"
      ],
      "datePublished": "2026-01-05T12:10:00.000Z",
      "dateModified": "2026-01-05T12:10:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Teradar reveals its first terahertz-band vision sensor for cars",
    "url": "https://techcrunch.com/2026/01/05/teradar-reveals-its-first-terahertz-band-vision-sensor-for-cars/",
    "image": "https://techcrunch.com/wp-content/uploads/2026/01/teradar-summit.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T12:00:00.000Z",
    "description": "\nTwo months after coming out of stealth with a $150 million fundraise, Boston-based Teradar is showing off its first flagship terahertz sensor at Cons",
    "body": "\nTwo months after coming out of stealth with a $150 million fundraise, Boston-based Teradar is showing off its first flagship terahertz sensor at Consumer Electronics Show 2026 this week. The company is positioning the sensor, called Summit, as the first long-range, high-resolution sensor of its kind “designed for high performance in any type of weather, filling a critical gap left by legacy radar and lidar sensors.”\nThe sensor will start shipping in 2028 if Teradar can lock down contracts with automakers. If that happens, the company expects Summit will help enable those companies to add partial, or even full, autonomy features to their vehicles.\nTeradar’s whole approach is about leveraging the relatively unused terahertz band of the electromagnetic spectrum between microwaves and infrared. And it’s a solid-state sensor, meaning there are no moving parts. All this is meant to give Teradar’s sensor the best qualities of lidar and radar sensors, with few of the drawbacks.\nIt’s a potentially attractive proposition for automakers who might wince at the cost of lidars or the limitations of radars. Teradar says it’s already working to prove its tech with five top automakers from the U.S. and Europe, and three Tier 1 suppliers.\nTeradar’s impending entry into the market comes at a pivotal moment for automotive sensor suppliers. Leading U.S. lidar company Luminar just filed for bankruptcy protection in December after contracts with Volvo and Mercedes-Benz fell apart, as the automakers backed away from the tech.\n\nThose deals also crumbled partly because of low-cost competition from China, according to Luminar. Lidar adoption has been strong in China’s auto market and shows little sign of slowing up. In October, Chinese lidar company Hesai said it had built more than 1 million lidar sensors in 2025.\nOther U.S. companies in the space, like Ouster (acquired and merged with rival Velodyne after a wave of consolidation) have diversified into potential markets like robotics and smart infrastructure.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nTeradar is looking beyond automotive, too, and that vision was reflected in its recent raise. The $150 million Series B included funding from Lockheed Martin’s venture arm, and VXI Capital, a new defense-focused fund led by the former CTO of the U.S. military’s Defense Innovation Unit.\nLidar isn’t completely dead in the auto industry, however. Rivian in December said it would integrate a roof-mounted lidar sensor (from an unnamed supplier) in its upcoming R2 SUV, signaling that there is still an appetite for using advanced sensor technology to bring autonomy to passenger vehicles, especially if it’s affordable.\nTeradar CEO Matt Carey told TechCrunch in November that he believes his terahertz sensor can meet all those marks, and sounded eager to pounce on the opportunity. \n“Our main job is to make sure our sensor gets on all automobiles, and whatever the best way to do that is, that’s what we’re going to pursue,” he said.\n\n\n\t\tSean O’Kane is a reporter who has spent a decade covering the rapidly-evolving business and technology of the transportation industry, including Tesla and the many startups chasing Elon Musk. Most recently, he was a reporter at Bloomberg News where he helped break stories about some of the most notorious EV SPAC flops. He previously worked at The Verge, where he also covered consumer technology, hosted many short- and long-form videos, performed product and editorial photography, and once nearly passed out in a Red Bull Air Race plane.\r\nYou can contact or verify outreach from Sean by emailing sean.okane@techcrunch.com or via encrypted message at okane.01 on Signal.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Teradar reveals its first terahertz-band vision sensor for cars",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2026/01/teradar-summit.jpg?w=150"
      ],
      "datePublished": "2026-01-05T12:00:00.000Z",
      "dateModified": "2026-01-05T12:00:00.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Microsoft’s Nadella wants us to stop thinking of AI as ‘slop’",
    "url": "https://techcrunch.com/2026/01/05/microsofts-nadella-wants-us-to-stop-thinking-of-ai-as-slop/",
    "image": "https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706501.jpg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T23:09:56.000Z",
    "description": "\nA couple of weeks after Merriam-Webster named “slop” as its word of the year, Microsoft CEO Satya Nadella weighed in on what to expect from AI in 202",
    "body": "\nA couple of weeks after Merriam-Webster named “slop” as its word of the year, Microsoft CEO Satya Nadella weighed in on what to expect from AI in 2026.\nIn his classic, intellectual style, Nadella wrote on his personal blog that he wants us to stop thinking of AI as “slop” and start thinking of it as “bicycles for the mind.”\nHe wrote, “A new concept that evolves ‘bicycles for the mind’ such that we always think of AI as a scaffolding for human potential vs a substitute.” \nHe continued: “We need to get beyond the arguments of slop vs sophistication and develop a new equilibrium in terms of our ‘theory of the mind’ that accounts for humans being equipped with these new cognitive amplifier tools as we relate to each other.”\nIf you parse through those syllables, you may see that he’s not only urging everyone to stop thinking of AI-generated content as slop, but also wants the tech industry to stop talking about AI as a replacement for humans. He hopes the industry will start talking about it as a human-helper productivity tool instead.\nHere’s the problem with that framing, though: Much of AI agent marketing uses the idea of replacing human labor as a way to price it, and justify its expense.\nMeanwhile, some of the biggest names in AI have been sounding the alarm that the tech will soon cause very high levels of human unemployment. For instance, in May Anthropic CEO Dario Amodei warned that AI could take away half of all entry-level white-collar jobs, raising unemployment to 10-20% over the next five years, and he doubled down on that last month in an interview on 60 Minutes.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nYet we currently don’t know how true such doomsday stats are. As Nadella implies, most AI tools today don’t replace workers, they are used by them (as long as the human doesn’t mind checking the AI’s work for accuracy).\nOne oft-cited research study is MIT’s ongoing Project Iceberg, which seeks to measure the economic impact on jobs as AI enters the workforce. Project Iceberg estimates that AI is currently capable of performing about 11.7% of human paid labor.\nWhile this has been widely reported as AI being capable of replacing nearly 12% of jobs, the Project says what it’s actually estimating is how much of a job can be offloaded to AI. It then calculates wages attached to that offloaded work. Interestingly, the tasks it cites as examples include automated paperwork for nurses and AI-written computer code.\nThat’s not to say there are no jobs being heavily impacted by AI. Corporate graphic artists and marketing bloggers are two examples, according to a Substack called Blood in the Machine. Then there are the high unemployment rates among new-grad junior coders.\nBut it’s also true that highly skilled artists, writers, and programmers produce better work with AI tools than those without the skills. AI can’t replace human creativity, yet.\nSo it’s perhaps no surprise that as we slide into 2026, some data is emerging that shows the jobs where AI has made the most progress are actually flourishing. Vanguard’s 2026 economic forecast report found that “the approximately 100 occupations most exposed to AI automation are actually outperforming the rest of the labor market in terms of job growth and real wage increases.”\nThe Vanguard report concludes that those who are masterfully using AI are making themselves more valuable, not replaceable. \nThe irony is that Microsoft’s own actions last year helped give rise to the AI-is-coming-for-our-jobs narrative. The company laid off over 15,000 people in 2025, even as it recorded record revenues and profits for its last fiscal year, which closed in June — citing success with AI as a reason. Nadella even wrote a public memo about the layoffs after these results.\nNotably, he didn’t say that internal AI efficiency led to cuts. But he did say that Microsoft had to “reimagine our mission for a new era” and named “AI transformation” as one of the company’s three business objectives in this era (the other two being security and quality).\nThe truth about job loss attributed to AI during 2025 is more nuanced. As the Vanguard report points out, this had less to do with internal AI efficiency and more to do with ordinary business practices that are less exciting to investors, like ending investment in slowing areas to pile in to growing ones.\nTo be fair, Microsoft wasn’t alone in laying off workers while pursuing AI. The technology was said to be responsible for almost 55,000 layoffs in the U.S. in 2025, according to research from firm Challenger, Gray & Christmas, CNBC reported. That report cited the large cuts last year at Amazon, Salesforce, Microsoft, and other tech companies chasing AI.\nAnd to be fair to slop, those of us who spend more time than we should on social media laughing at memes and AI-generated short-form videos might argue that slop is one of AI’s most entertaining (if not best) uses, too.\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Microsoft’s Nadella wants us to stop thinking of AI as ‘slop’",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706501.jpg?w=150"
      ],
      "datePublished": "2026-01-05T23:09:56.000Z",
      "dateModified": "2026-01-05T23:09:56.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  },
  {
    "title": "Offshore wind developers sue Trump administration for halting $25B in projects",
    "url": "https://techcrunch.com/2026/01/05/offshore-wind-developers-sue-trump-administration-for-halting-25b-in-projects/",
    "image": "https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-831764682.jpeg?w=150",
    "tag": "Tech",
    "date": "2026-01-05T18:20:23.000Z",
    "description": "\nThree offshore wind developers are suing the Trump administration after the Department of the Interior halted five projects worth a total of $25 bill",
    "body": "\nThree offshore wind developers are suing the Trump administration after the Department of the Interior halted five projects worth a total of $25 billion on December 22. If completed, the projects would generate a total of 6 gigawatts of electricity.\nTwo lawsuits were filed Thursday and Friday last week by Ørsted and Equinor, which are developing the 704 megawatt Revolution Wind and the 2 gigawatt Empire Wind, respectively. Another was filed on December 23 by Dominion Energy, which is building a 2.6 gigawatt farm off the coast of Virginia.\nRevolution Wind is nearly 90% complete, while Empire Wind and Coastal Virginia Offshore Wind are each about 60% complete. Dominion said it was losing $5 million per day as a result of the halt.\nAvangrid, which is developing Vineyard Wind 1, has not filed a lawsuit yet. Nearly half that project is currently operational.\nThe Department of the Interior cited national security concerns in its decision to stop construction on the projects. Though it didn’t mention specifics, the Trump administration may have been referencing the challenges wind turbines present to radar operations. The Department of Energy had issued a report that discussed this security concern, as well as solutions to it, in February 2024.\nWind turbines’ whirling blades have been known to stymie radar systems, but researchers in the government and private companies have been working to mitigate the problem for well over a decade. \nChoosing the precise site for wind energy projects is one of the biggest ways to ameliorate interference. The Bureau of Ocean Energy Management coordinates with the Military Aviation and Installation Assurance Siting Clearinghouse to “review each proposed offshore wind project on a project-by-project basis, and would attempt to de-conflict concerns related to individual projects or multiple projects,” according to Vineyard Wind 1’s environmental impact statement.\n\n\t\tTechcrunch event\n\t\t\n\t\t\tSan Francisco\n\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 13-15, 2026\n\t\t\t\t\t\t\t\n\t\t\n\t\nNewer radar systems can filter the noise that wind farms produce through adaptive processing algorithms, Rand Corporation senior engineer Nicholas O’Donoughue previously told TechCrunch. Vineyard Wind 1 agreed to help fund radar adaptations and to curtail operations when asked by the Pentagon, for example. \nEarlier last year, the Trump administration halted approvals for new offshore wind projects in addition to pausing work on Empire Wind and Revolution Wind. The latter restarted after New York State negotiated with the Trump administration, while a federal judge struck down the stop work order for Revolution Wind. \n\n\n\t\tTim De Chant is a senior climate reporter at TechCrunch. He has written for a wide range of publications, including Wired magazine, the Chicago Tribune, Ars Technica, The Wire China, and NOVA Next, where he was founding editor. \r\nDe Chant is also a lecturer in MIT’s Graduate Program in Science Writing, and he was awarded a Knight Science Journalism Fellowship at MIT in 2018, during which time he studied climate technologies and explored new business models for journalism. He received his PhD in environmental science, policy, and management from the University of California, Berkeley, and his BA degree in environmental studies, English, and biology from St. Olaf College.\r\nYou can contact or verify outreach from Tim by emailing tim.dechant@techcrunch.com.\t\n\t\n\t\tView Bio \n\t\n",
    "schema": {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "Offshore wind developers sue Trump administration for halting $25B in projects",
      "image": [
        "https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-831764682.jpeg?w=150"
      ],
      "datePublished": "2026-01-05T18:20:23.000Z",
      "dateModified": "2026-01-05T18:20:23.000Z",
      "author": [
        {
          "@type": "Person",
          "name": "Midnight Ink Staff"
        }
      ]
    }
  }
]